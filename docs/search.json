[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Set Size Supplement",
    "section": "",
    "text": "This repository contains all supplementary materials, code, and analyses for the paper “Estimating the Size of a Set Using Cascading Exclusion.” by Sourav Chatterjee, Persi Diaconis and Susan Holmes, August 2025."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Set Size Supplement",
    "section": "",
    "text": "This repository contains all supplementary materials, code, and analyses for the paper “Estimating the Size of a Set Using Cascading Exclusion.” by Sourav Chatterjee, Persi Diaconis and Susan Holmes, August 2025."
  },
  {
    "objectID": "index.html#navigation",
    "href": "index.html#navigation",
    "title": "Set Size Supplement",
    "section": "Navigation",
    "text": "Navigation\n\nConvex Hulls: Geometric approaches to set size estimation\nCoincidence Detection: Distance-based methods and Anderson-Darling analysis\nCoincidence Detection on SILVA data: Detection of coincident bacterial species from SILVA data example\nPrediction Sets: Regression-based estimation approaches"
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "Set Size Supplement",
    "section": "Data",
    "text": "Data\n\nsilva_nr99_v138.2_toGenus_trainset.fa: Original SILVA database version\nsilva_aligned_bacteria_sequences.fasta: Aligned bacterial DNA sequences\nsilva_sequence_info.csv: Sequence metadata and taxonomic information"
  },
  {
    "objectID": "index.html#reproducibility",
    "href": "index.html#reproducibility",
    "title": "Set Size Supplement",
    "section": "Reproducibility",
    "text": "Reproducibility\nClone this repository and run render or build."
  },
  {
    "objectID": "ConvexHull.html",
    "href": "ConvexHull.html",
    "title": "Convex Hull",
    "section": "",
    "text": "Code\n# Rectangle [0,4] × [0,2] with 15 extreme points\nreconstruct_rectangle_calculation &lt;- function() {\n  cat(\"=== CONVEX HULL AREA ESTIMATE FOR RECTANGLE [0,4] × [0,2] ===\\n\\n\")\n  \n  # The correct setup from our original work:\n  cat(\"Original problem:\\n\")\n  cat(\"- Region K: Rectangle [0,4] × [0,2]\\n\")\n  cat(\"- True area of K: 4 × 2 = 8\\n\")\n  cat(\"- Sample size: n = 100\\n\")\n  cat(\"- Observed extreme points: V_n = 15\\n\\n\")\n  \n  # Generate example with rectangle\n  set.seed(123)\n  n &lt;- 100\n  d &lt;- 2\n  \n  # Generate uniform points in rectangle [0,4] × [0,2]\n  points &lt;- matrix(0, nrow = n, ncol = d)\n  points[,1] &lt;- runif(n, 0, 4)  # x coordinates in [0,4]\n  points[,2] &lt;- runif(n, 0, 2)  # y coordinates in [0,2]\n  \n  cat(\"Generated\", n, \"uniform points in rectangle [0,4] × [0,2]\\n\")\n  \n  # Find convex hull\n  hull_indices &lt;- chull(points[,1], points[,2])\n  hull_vertices &lt;- points[hull_indices, ]\n  V_n &lt;- length(hull_indices)\n  \n  cat(\"- Number of extreme points V_n =\", V_n, \"\\n\")\n  \n  # Compute convex hull area using shoelace formula\n  n_hull &lt;- nrow(hull_vertices)\n  area &lt;- 0\n  for(i in 1:n_hull) {\n    j &lt;- ifelse(i == n_hull, 1, i + 1)\n    area &lt;- area + (hull_vertices[i,1] * hull_vertices[j,2] - hull_vertices[j,1] * hull_vertices[i,2])\n  }\n  vol_hull &lt;- abs(area) / 2\n  \n  cat(\"- Convex hull area =\", format(vol_hull, digits = 6), \"\\n\\n\")\n  \n  # Apply the volume estimator formula\n  vol_hat &lt;- vol_hull / (1 - V_n/n)\n  \n  cat(\"Volume estimation:\\n\")\n  cat(\"- Formula: vol_hat = vol_hull / (1 - V_n/n)\\n\")\n  cat(\"- Calculation: vol_hat =\", format(vol_hull, digits = 6), \"/ (1 -\", V_n, \"/\", n, \")\\n\")\n  cat(\"                      =\", format(vol_hull, digits = 6), \"/\", format(1 - V_n/n, digits = 4), \"\\n\")\n  cat(\"                      =\", format(vol_hat, digits = 6), \"\\n\\n\")\n  \n  # Compare with true area\n  true_area &lt;- 8\n  rel_error &lt;- abs(vol_hat - true_area) / true_area\n  \n  cat(\"Results:\\n\")\n  cat(\"- True rectangle area:\", true_area, \"\\n\")\n  cat(\"- Estimated area:\", format(vol_hat, digits = 6), \"\\n\")\n  cat(\"- Relative error:\", round(rel_error, 4), \"\\n\")\n  cat(\"- Error percentage:\", round(rel_error * 100, 2), \"%\\n\\n\")\n  \n  # Show the hull vertices\n  cat(\"Convex hull vertices:\\n\")\n  print(round(hull_vertices, 3))\n  \n  return(list(\n    true_area = true_area,\n    hull_area = vol_hull,\n    V_n = V_n,\n    estimated_area = vol_hat,\n    rel_error = rel_error\n  ))\n}\n\n# Show what the convex hull typically looks like for rectangle\nanalyze_rectangle_hull &lt;- function() {\n  cat(\"\\n=== ANALYSIS: CONVEX HULL OF RECTANGLE ===\\n\\n\")\n  \n  cat(\"For uniform points in rectangle [0,4] × [0,2]:\\n\")\n  cat(\"- Most points will be interior\\n\")\n  cat(\"- Extreme points will be near the boundary\\n\")\n  cat(\"- Convex hull will approximate the rectangle\\n\")\n  cat(\"- With enough points, hull area → rectangle area (8)\\n\\n\")\n  \n  # Show typical behavior\n  n_trials &lt;- 10\n  results &lt;- matrix(0, nrow = n_trials, ncol = 4)\n  colnames(results) &lt;- c(\"V_n\", \"Hull_Area\", \"Estimated_Area\", \"Error\")\n  \n  for(i in 1:n_trials) {\n    # Generate points in rectangle\n    n &lt;- 100\n    points &lt;- matrix(0, nrow = n, ncol = 2)\n    points[,1] &lt;- runif(n, 0, 4)\n    points[,2] &lt;- runif(n, 0, 2)\n    \n    # Get hull\n    hull_indices &lt;- chull(points[,1], points[,2])\n    hull_vertices &lt;- points[hull_indices, ]\n    V_n &lt;- length(hull_indices)\n    \n    # Compute area\n    n_hull &lt;- nrow(hull_vertices)\n    area &lt;- 0\n    for(j in 1:n_hull) {\n      k &lt;- ifelse(j == n_hull, 1, j + 1)\n      area &lt;- area + (hull_vertices[j,1] * hull_vertices[k,2] - hull_vertices[k,1] * hull_vertices[j,2])\n    }\n    vol_hull &lt;- abs(area) / 2\n    \n    # Estimate\n    vol_hat &lt;- vol_hull / (1 - V_n/n)\n    error &lt;- abs(vol_hat - 8) / 8\n    \n    results[i, ] &lt;- c(V_n, vol_hull, vol_hat, error)\n  }\n  \n  cat(\"Results from\", n_trials, \"trials:\\n\")\n  print(round(results, 4))\n  \n  cat(\"\\nSummary:\\n\")\n  cat(\"- Mean V_n:\", round(mean(results[,1]), 1), \"\\n\")\n  cat(\"- Mean hull area:\", round(mean(results[,2]), 3), \"\\n\")\n  cat(\"- Mean estimated area:\", round(mean(results[,3]), 3), \"\\n\")\n  cat(\"- Mean relative error:\", round(mean(results[,4]), 4), \"\\n\")\n}\n\n# Run the analysis\nresult &lt;- reconstruct_rectangle_calculation()\n\n\n=== CONVEX HULL AREA ESTIMATE FOR RECTANGLE [0,4] × [0,2] ===\n\nOriginal problem:\n- Region K: Rectangle [0,4] × [0,2]\n- True area of K: 4 × 2 = 8\n- Sample size: n = 100\n- Observed extreme points: V_n = 15\n\nGenerated 100 uniform points in rectangle [0,4] × [0,2]\n- Number of extreme points V_n = 13 \n- Convex hull area = 6.47026 \n\nVolume estimation:\n- Formula: vol_hat = vol_hull / (1 - V_n/n)\n- Calculation: vol_hat = 6.47026 / (1 - 13 / 100 )\n                      = 6.47026 / 0.87 \n                      = 7.43708 \n\nResults:\n- True rectangle area: 8 \n- Estimated area: 7.43708 \n- Relative error: 0.0704 \n- Error percentage: 7.04 %\n\nConvex hull vertices:\n       [,1]  [,2]\n [1,] 3.977 0.440\n [2,] 3.599 0.285\n [3,] 2.710 0.121\n [4,] 1.655 0.021\n [5,] 0.555 0.462\n [6,] 0.488 0.493\n [7,] 0.098 1.042\n [8,] 0.002 1.493\n [9,] 0.168 1.908\n[10,] 1.374 1.971\n[11,] 2.834 1.968\n[12,] 3.532 1.909\n[13,] 3.827 1.871\n\n\nCode\nanalyze_rectangle_hull()\n\n\n\n=== ANALYSIS: CONVEX HULL OF RECTANGLE ===\n\nFor uniform points in rectangle [0,4] × [0,2]:\n- Most points will be interior\n- Extreme points will be near the boundary\n- Convex hull will approximate the rectangle\n- With enough points, hull area → rectangle area (8)\n\nResults from 10 trials:\n      V_n Hull_Area Estimated_Area  Error\n [1,]  14    7.0699         8.2209 0.0276\n [2,]  13    7.2534         8.3372 0.0421\n [3,]  13    7.1736         8.2455 0.0307\n [4,]  11    6.9194         7.7747 0.0282\n [5,]  11    6.8912         7.7429 0.0321\n [6,]  13    7.3092         8.4013 0.0502\n [7,]  15    7.0385         8.2806 0.0351\n [8,]   9    7.4041         8.1363 0.0170\n [9,]  12    7.3324         8.3323 0.0415\n[10,]  11    7.2956         8.1973 0.0247\n\nSummary:\n- Mean V_n: 12.2 \n- Mean hull area: 7.169 \n- Mean estimated area: 8.167 \n- Mean relative error: 0.0329 \n\n\nCode\ncat(\"\\n=== SUMMARY ===\\n\")\n\n\n\n=== SUMMARY ===\n\n\nCode\ncat(\"For the rectangle [0,4] × [0,2] with ~15 extreme points:\\n\")\n\n\nFor the rectangle [0,4] × [0,2] with ~15 extreme points:\n\n\nCode\ncat(\"- We computed the convex hull area using the shoelace formula\\n\")\n\n\n- We computed the convex hull area using the shoelace formula\n\n\nCode\ncat(\"- Applied the estimator: area_estimate = hull_area / (1 - 15/100)\\n\")\n\n\n- Applied the estimator: area_estimate = hull_area / (1 - 15/100)\n\n\nCode\ncat(\"- This gave estimates typically close to the true area of 8\\n\")\n\n\n- This gave estimates typically close to the true area of 8\n\n\nCode\ncat(\"- The method worked well for the 2D rectangle case\\n\")\n\n\n- The method worked well for the 2D rectangle case"
  },
  {
    "objectID": "analysis/03-bacterialdata.html",
    "href": "analysis/03-bacterialdata.html",
    "title": "Coincidence Detection using Distances: SILVA data",
    "section": "",
    "text": "This analysis creates the data we analyse from the SILVA database by taking a random set of 205 sequences to start with, aligning them and choosing the most informative contiguous subsequence of length 450. We take 200 first ones that align correctly and then use those to run the 40/160 cuts many times, we look at the 1st and 5th percentiles. Using the 1st percentil we create a threshold of detection of “very close” species and see that in fact these are all duplicates."
  },
  {
    "objectID": "analysis/03-bacterialdata.html#overview",
    "href": "analysis/03-bacterialdata.html#overview",
    "title": "Coincidence Detection using Distances: SILVA data",
    "section": "",
    "text": "This analysis creates the data we analyse from the SILVA database by taking a random set of 205 sequences to start with, aligning them and choosing the most informative contiguous subsequence of length 450. We take 200 first ones that align correctly and then use those to run the 40/160 cuts many times, we look at the 1st and 5th percentiles. Using the 1st percentil we create a threshold of detection of “very close” species and see that in fact these are all duplicates."
  },
  {
    "objectID": "analysis/03-bacterialdata.html#step-1-prepare-downloaded-data-from-silva-database",
    "href": "analysis/03-bacterialdata.html#step-1-prepare-downloaded-data-from-silva-database",
    "title": "Coincidence Detection using Distances: SILVA data",
    "section": "Step 1: Prepare downloaded data from SILVA database",
    "text": "Step 1: Prepare downloaded data from SILVA database\nWe start by loading the libraries we need, include the Bioconductor package DECIPHER that we use for alignment. To install the Bioconductor package DECIPHER that provides the best alignment function:\nBiocManager::install(\"DECIPHER\")\nWe make sure Biostrings is loaded last because of the width function.\nThe Silva database silva_nr99_v138.2_toGenus_trainset.fa was downloaded from zenodo (DADA2) benchmark dataset.\nOriginally this file should contain only bacteria, but we will check.\n\n\nCode\nlibrary(dplyr)\nlibrary(DECIPHER)\nlibrary(seqinr)\nlibrary(ape)\nlibrary(ggplot2)\nlibrary(Biostrings)\n\n\n\n\nCode\n# Step 1: Load the SILVA database\nsilva_file &lt;- \"../data/silva_nr99_v138.2_toGenus_trainset.fa\"\nsilva_seqs &lt;- readDNAStringSet(silva_file)\n\n\n\nAmong the Bacteria we pick the top 10 phyla\n\n\nCode\n# Step 2: Filter for bacterial sequences and extract taxonomic information\nis_bacteria &lt;- grepl(\"Bacteria;\", names(silva_seqs))\nbacteria_seqs &lt;- silva_seqs[is_bacteria]\n\n# Extract phylum information\nget_phylum &lt;- function(full_taxonomy) {\n  split_tax &lt;- strsplit(full_taxonomy, \";\")[[1]]\n  return(ifelse(length(split_tax) &gt;= 3, split_tax[3], NA))  # Return NA if phylum level is missing\n}\n\nphyla &lt;- sapply(names(bacteria_seqs), get_phylum)\n\n# Step 3: Stratify sampling across phyla\nphylum_counts &lt;- table(phyla, useNA = \"ifany\")\ntop_phyla &lt;- names(sort(phylum_counts, decreasing = TRUE))\ntop_phyla &lt;- top_phyla[!is.na(top_phyla)][1:10]  # Top 10 phyla, excluding NA\n\n\nWe choose to sample 21 species from the 10 most frequent phyla to create a diverse reference set of about 210 (we’ll discard a few when doing the alignment).\n\n\nCode\n# Calculate number of sequences to sample from each phylum\nn_per_phylum &lt;- 21  # Increased from 20 to 21 to get 210 sequences (allowing for some potential loss in alignment)\n\n\n\n\nWe create the random sample from the 10 main phyla.\n\n\nCode\n# Step 4: Random sampling\nset.seed(2042)  # for reproducibility\n\nsampled_seqs &lt;- lapply(top_phyla, function(phylum) {\n  phylum_indices &lt;- which(phyla == phylum)\n  if (length(phylum_indices) &gt; n_per_phylum) {\n    sampled_indices &lt;- sample(phylum_indices, n_per_phylum)\n  } else {\n    sampled_indices &lt;- phylum_indices\n  }\n  bacteria_seqs[sampled_indices]\n})\n\n# Combine sampled sequences\nfinal_sample &lt;- do.call(c, sampled_seqs)\n\n# Ensure we have at least 205 sequences, randomly sampling more if needed\nif (length(final_sample) &lt; 205) {\n  remaining_indices &lt;- which(!names(bacteria_seqs) %in% names(final_sample))\n  additional_sample &lt;- sample(bacteria_seqs[remaining_indices], 205 - \n                                length(final_sample))\n  final_sample &lt;- c(final_sample, additional_sample)\n}\n\n# Trim to exactly 205 if we have more\nfinal_sample &lt;- final_sample[1:205]\ncat(\"\\nSummary of width of sequences chosen:\",\nsummary(Biostrings::width(final_sample)),\"\\n\")\n\n\n\nSummary of width of sequences chosen: 1200 1364 1431 1419.634 1482 1870"
  },
  {
    "objectID": "analysis/03-bacterialdata.html#step-2-alignment-with-penalties",
    "href": "analysis/03-bacterialdata.html#step-2-alignment-with-penalties",
    "title": "Coincidence Detection using Distances: SILVA data",
    "section": "Step 2: Alignment with penalties",
    "text": "Step 2: Alignment with penalties\nSo the sequences have varying lengths from about 1200 to rmax(Biostrings::width(final_sample))` and we want to find subsequences of length about 450 that can be “aligned” so have a certain number of conserved positions that can be used as anchors and we need variability in the other positions which serve to characterize the different species.\n\n\nCode\n# Align the sequences\ncat(\"Aligning sequences...\\n\")\n\n\nAligning sequences...\n\n\nCode\naligned_seqs &lt;- AlignSeqs(final_sample, anchor=NA, verbose = FALSE)\n\n# Convert to DNAStringSet for easier manipulation\naligned_seqs_dna &lt;- DNAStringSet(aligned_seqs)\n\n# Function to find the most informative region\nfind_informative_region &lt;- function(aligned_seqs, target_length = 450, window_size = 50) {\n  # Create a simple consensus\n  cons_matrix &lt;- consensusMatrix(aligned_seqs)\n  consensus &lt;- apply(cons_matrix[c(\"A\", \"C\", \"G\", \"T\"), ], 2, function(x) {\n    if (sum(x) == 0) return(\"-\")\n    names(which.max(x))\n  })\n  consensus &lt;- paste(consensus, collapse = \"\")\n  \n  # Slide a window across the consensus sequence\n  scores &lt;- sapply(1:(nchar(consensus) - window_size + 1), function(i) {\n    window &lt;- substr(consensus, i, i + window_size - 1)\n    sum(window != \"-\") / window_size  # Score based on non-gap content\n  })\n  \n  # Find the highest scoring region that's close to the target length\n  start &lt;- which.max(scores)\n  end &lt;- min(start + target_length - 1, nchar(consensus))\n  \n  return(c(start, end))\n}\n\n# Find the most informative region\ncat(\"Finding informative region...\\n\")\n\n\nFinding informative region...\n\n\nCode\nregion &lt;- find_informative_region(aligned_seqs_dna)\n\n# Extract the region from all sequences\nfinal_aligned_seqs &lt;- subseq(aligned_seqs_dna, start = region[1], end = region[2])\n\n# Create a data frame to store original names and new unique identifiers\nsequence_info &lt;- data.frame(\n  original_name = names(final_aligned_seqs),\n  unique_id = paste0(\"Seq_\", seq_along(final_aligned_seqs)),\n  stringsAsFactors = FALSE\n)\n\n# Assign unique identifiers as names, but keep original names as attributes\nnames(final_aligned_seqs) &lt;- sequence_info$unique_id\nfor (i in seq_along(final_aligned_seqs)) {\n  attr(final_aligned_seqs[[i]], \"original_name\") &lt;- sequence_info$original_name[i]\n}\n\n# Write the aligned sequences to a file\nwriteXStringSet(final_aligned_seqs, \"../data/test_silva_aligned_bacteria_sequences.fasta\")\n\n# Write sequence information to a separate file\nwrite.csv(sequence_info, \"../data/test_silva_sequence_info.csv\", row.names = FALSE)\n\n# Print summary of the final aligned sequences\ncat(\"\\nFinal alignment summary:\\n\")\n\n\n\nFinal alignment summary:\n\n\nCode\nprint(summary(Biostrings::width(final_aligned_seqs)))\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    450     450     450     450     450     450 \n\n\nCode\ncat(\"\\nNumber of sequences in final alignment:\", length(final_aligned_seqs), \"\\n\")\n\n\n\nNumber of sequences in final alignment: 205 \n\n\nCode\n# Print some additional information about the alignment\ncat(\"\\nAlignment characteristics:\\n\")\n\n\n\nAlignment characteristics:\n\n\nCode\ncons_matrix &lt;- consensusMatrix(final_aligned_seqs)\ncat(\"Conserved positions:\", sum(apply(cons_matrix[c(\"A\", \"C\", \"G\", \"T\"), ], 2, function(x) max(x) == sum(x))), \"\\n\")\n\n\nConserved positions: 79 \n\n\nCode\ncat(\"Variable positions:\", sum(apply(cons_matrix[c(\"A\", \"C\", \"G\", \"T\"), ], 2, function(x) sum(x &gt; 0) &gt; 1)), \"\\n\")\n\n\nVariable positions: 371 \n\n\nCode\n# Print information about the gap content in the final sequences\ngap_props_final &lt;- rowSums(as.matrix(final_aligned_seqs) == \"-\") / Biostrings::width(final_aligned_seqs)[1]\ncat(\"\\nGap proportion in final sequences:\\n\")\n\n\n\nGap proportion in final sequences:\n\n\nCode\nprint(summary(gap_props_final))\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.2467  0.3000  0.3422  0.3524  0.3800  0.7756 \n\n\nCode\ncat(\"Max gap proportion:\", max(gap_props_final), \"\\n\")\n\n\nMax gap proportion: 0.7755556 \n\n\nCode\n# Print a few examples of original names and their corresponding unique IDs\ncat(\"\\nExample of sequence names:\\n\")\n\n\n\nExample of sequence names:\n\n\nCode\nprint(head(sequence_info))\n\n\n                                                                                          original_name\n1                            Bacteria;Pseudomonadota;Gammaproteobacteria;Pseudomonadales;Moraxellaceae;\n2 Bacteria;Pseudomonadota;Gammaproteobacteria;Enterobacterales;Enterobacteriaceae;Escherichia-Shigella;\n3             Bacteria;Pseudomonadota;Gammaproteobacteria;Pseudomonadales;Pseudomonadaceae;Pseudomonas;\n4       Bacteria;Pseudomonadota;Gammaproteobacteria;Burkholderiales;Oxalobacteraceae;Janthinobacterium;\n5                 Bacteria;Pseudomonadota;Gammaproteobacteria;Pseudomonadales;Halomonadaceae;Halomonas;\n6                              Bacteria;Pseudomonadota;Gammaproteobacteria;Pseudomonadales;SAR86 clade;\n  unique_id\n1     Seq_1\n2     Seq_2\n3     Seq_3\n4     Seq_4\n5     Seq_5\n6     Seq_6"
  },
  {
    "objectID": "analysis/03-bacterialdata.html#step-3-compute-the-within-and-between-distances-and-compare-the-histograms",
    "href": "analysis/03-bacterialdata.html#step-3-compute-the-within-and-between-distances-and-compare-the-histograms",
    "title": "Coincidence Detection using Distances: SILVA data",
    "section": "Step 3: Compute the within and between distances and compare the histograms",
    "text": "Step 3: Compute the within and between distances and compare the histograms\nNow we have the 200 sequences, we can do the simulations by making random splits 40-160 here we use the Kimura distance between DNA sequences.\n\n\nCode\nlibrary(ape)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(Biostrings)\n\n# Read the aligned sequences\naligned_seqs &lt;- readDNAStringSet(\"../data/silva_aligned_bacteria_sequences.fasta\")\n\n# Take the first 200 sequences\naligned_seqs_200 &lt;- aligned_seqs[1:200]\n\n# Convert to DNAbin object (required for dist.dna)\ndna_bin &lt;- as.DNAbin(aligned_seqs_200)\n\n# Compute Kimura 2-parameter distances\ndist_matrix &lt;- as.matrix(dist.dna(dna_bin, model = \"K80\", pairwise.deletion = TRUE))\n\n# Function to get nearest neighbor distances\nget_nn_distances &lt;- function(dist_matrix, indices) {\n  nn_distances &lt;- apply(dist_matrix[indices, indices, drop = FALSE], 1, function(row) min(row[row &gt; 0]))\n  return(nn_distances)\n}\n\n# Function to get nearest neighbor distances between two sets\nget_nn_distances_between &lt;- function(dist_matrix, from_indices, to_indices) {\n  nn_distances &lt;- apply(dist_matrix[from_indices, to_indices, drop = FALSE], 1, min)\n  return(nn_distances)\n}\n\n# Perform multiple samplings and collect results\nn_simulations &lt;- 1000\nsample_size &lt;- 40\nwithin_sample_distances &lt;- list()\nbetween_sample_distances &lt;- list()\n\n\nset.seed(194501)  # for reproducibility\n\nfor (i in 1:n_simulations) {\n  # Random sampling\n  sample_indices &lt;- sample(1:200, sample_size)\n  non_sample_indices &lt;- setdiff(1:200, sample_indices)\n  \n  # Within-sample nearest neighbor distances\n  within_sample_distances[[i]] &lt;- get_nn_distances(dist_matrix, sample_indices)\n  \n  # Between sample and population nearest neighbor distances\n  between_sample_distances[[i]] &lt;- get_nn_distances_between(dist_matrix, non_sample_indices, sample_indices)\n}\n\n# Combine results\nwithin_sample_df &lt;- data.frame(\n  Distance = unlist(within_sample_distances),\n  Type = \"Within Sample\"\n)\n\nbetween_sample_df &lt;- data.frame(\n  Distance = unlist(between_sample_distances),\n  Type = \"Between Sample and Population\"\n)\n\n\n\nShow the histograms of both nn distances\n\n\nCode\nall_distances_df &lt;- rbind(within_sample_df, between_sample_df)\n\n# Create histograms\nall_plot&lt;- ggplot(all_distances_df, aes(x = Distance, fill = Type)) +\n  geom_histogram(position = \"identity\", alpha = 0.7, bins = 30) +\n  facet_wrap(~Type, scales = \"free_y\", ncol = 1) +\n  labs(# title = \"Distribution of Nearest Neighbor Distances\",\n       x = \"Kimura 2-parameter Distance\",\n       y = \"Frequency\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"Within Sample\" = \"blue\", \"Between Sample and Population\" = \"red\")) +\n   theme(legend.position = \"none\",\n          #axis.title = element_blank(),\n          plot.title = element_blank())\n\nprint(all_plot)\n\n\n\n\n\n\n\n\n\nCode\n#ggsave(\"silva_nearest_neighbor_distances_histogram.png\", all_distances_df , width = 6, height = 4)\n\n\n\n\nCode\nlibrary(gridExtra)\n# Function to create histogram without legend or title\ncreate_histogram &lt;- function(data, fill_color) {\n  ggplot(data.frame(Distance = data), aes(x = Distance)) +\n    geom_histogram(bins = 30, fill = fill_color, alpha = 0.7) +\n    labs(# title = \"Distribution of Nearest Neighbor Distances\",\n       x = \"Kimura 2-parameter Distance\",\n       y = \"Frequency\") +\n    theme_minimal() +\n    theme(legend.position = \"none\",\n          #axis.title = element_blank(),\n          plot.title = element_blank())\n}\n\n# Create histograms\nhist_within &lt;- create_histogram(unlist(within_sample_distances), \"blue\")\nhist_between &lt;- create_histogram(unlist(between_sample_distances), \"red\")\n\n# Combine histograms\ncombined_hist1 &lt;- grid.arrange(hist_within, hist_between, nrow = 2)\n\n\n\n\n\n\n\n\n\nCode\nprint(combined_hist1)\n\n\nTableGrob (2 x 1) \"arrange\": 2 grobs\n  z     cells    name           grob\n1 1 (1-1,1-1) arrange gtable[layout]\n2 2 (2-2,1-1) arrange gtable[layout]\n\n\nCode\n# Save combined histogram\nggsave(\"silva_combined_distances_histogram.png\", combined_hist1, width = 5, height = 5)\n\n\n\n\nCode\n# Function to create histogram with density estimate\ncreate_histogram_with_density &lt;- function(data, fill_color, line_color) {\n  ggplot(data.frame(Distance = data), aes(x = Distance)) +\n    geom_histogram(aes(y = ..density..), bins = 30, fill = fill_color, alpha = 0.7) +\n    geom_density(color = line_color, size = 1, alpha = 0.8,adjust=2) +\n    theme_minimal() +\n    theme(legend.position = \"none\",\n          axis.title = element_blank(),\n          plot.title = element_blank())\n}\n\n# Create histograms with density estimates\nhist_within &lt;- create_histogram_with_density(unlist(within_sample_distances), \"blue\", \"darkblue\")\nhist_between &lt;- create_histogram_with_density(unlist(between_sample_distances), \"red\", \"darkred\")\n\n# Combine histograms\ncombined_hist2 &lt;- grid.arrange(hist_within, hist_between, nrow = 2)\n\n\n\n\n\n\n\n\n\nCode\nprint(combined_hist2)\n\n\nTableGrob (2 x 1) \"arrange\": 2 grobs\n  z     cells    name           grob\n1 1 (1-1,1-1) arrange gtable[layout]\n2 2 (2-2,1-1) arrange gtable[layout]\n\n\nCode\n# Save combined histogram\n# ggsave(\"silva_combined_dist_hist_density.png\", combined_hist2, width = 10, height = 5)"
  },
  {
    "objectID": "analysis/03-bacterialdata.html#step-4-using-distribution-percentiles-to-find-the-threshold-of-coincidence",
    "href": "analysis/03-bacterialdata.html#step-4-using-distribution-percentiles-to-find-the-threshold-of-coincidence",
    "title": "Coincidence Detection using Distances: SILVA data",
    "section": "Step 4: Using distribution percentiles to find the threshold of “coincidence”",
    "text": "Step 4: Using distribution percentiles to find the threshold of “coincidence”\n\n\nCode\n# Assuming within_sample_distances and between_sample_distances are lists of vectors\n# Function to calculate percentiles for a list of distance vectors\ncalculate_percentiles &lt;- function(distance_list, percentiles) {\n  all_distances &lt;- unlist(distance_list)\n  sapply(percentiles, function(p) quantile(all_distances, p/100))\n}\n\n# Calculate 1st and 5th percentiles for both distributions\npercentiles &lt;- c(1, 5)\nwithin_percentiles &lt;- calculate_percentiles(within_sample_distances, percentiles)\nbetween_percentiles &lt;- calculate_percentiles(between_sample_distances, percentiles)\n\n# Create a data frame for easy comparison\npercentile_comparison &lt;- data.frame(\n  Percentile = c(\"1st\", \"5th\"),\n  Within_Sample = within_percentiles,\n  Sample_to_Population = between_percentiles\n)\n\n# Print the comparison\ncat(\"Comparison of 1st and 5th percentiles:\\n\")\n\n\nComparison of 1st and 5th percentiles:\n\n\nCode\nprint(percentile_comparison, row.names = FALSE)\n\n\n Percentile Within_Sample Sample_to_Population\n        1st    0.01548814           0.01460938\n        5th    0.05423807           0.05096344\n\n\nCode\n# Calculate and print the differences\ncat(\"\\nDifferences (Within Sample - Sample to Population):\\n\")\n\n\n\nDifferences (Within Sample - Sample to Population):\n\n\nCode\ndiff_1st &lt;- within_percentiles[1] - between_percentiles[1]\ndiff_5th &lt;- within_percentiles[2] - between_percentiles[2]\ncat(\"1st percentile difference:\", diff_1st, \"\\n\")\n\n\n1st percentile difference: 0.0008787655 \n\n\nCode\ncat(\"5th percentile difference:\", diff_5th, \"\\n\")\n\n\n5th percentile difference: 0.003274623 \n\n\n\n\nCode\n# Plot distribution of AD statistics\nad_plot&lt;- ggplot(data.frame(AD_statistic = ad_stats[, \"AD_statistic\"]), aes(x = AD_statistic)) +\n  geom_histogram(aes(y = ..density..), fill = \"purple\", alpha = 0.7, bins = 30) +\n  geom_density(color=\"purple\")+\n  geom_density(data = data.frame(stat = ad_stats_null),\n               aes(x = stat),\n               color = \"blue\", size = 1) +\n  labs(#title = \"Distribution of Anderson-Darling Test Statistics\",\n       x = \"AD Statistic\",\n       y = \"Frequency\") +\n  theme_minimal() +\n  theme(legend.position=\"none\",\n        plot.title = element_blank())\n\nprint(ad_plot)\n\n#ggsave(\"silva_anderson_darling_histogram.png\", ad_plot, width = 5, height = 5)\n\n\n\n\nCode\nlibrary(ape)\nlibrary(ggplot2)\nlibrary(Biostrings)\n\n# Read the aligned sequences\naligned_seqs &lt;- readDNAStringSet(\"../data/silva_aligned_bacteria_sequences.fasta\")\n\n# Take the first 200 sequences\naligned_seqs_200 &lt;- aligned_seqs[1:200]\n\n# Read the sequence information\nsequence_info &lt;- read.csv(\"../data/silva_sequence_info.csv\", stringsAsFactors = FALSE)\n\n# Create a named vector for easy lookup\nname_mapping &lt;- setNames(sequence_info$original_name, sequence_info$unique_id)\n\n# Convert to DNAbin object (required for dist.dna)\ndna_bin &lt;- as.DNAbin(aligned_seqs_200)\n\n# Compute Kimura 2-parameter distances\ndist_matrix &lt;- as.matrix(dist.dna(dna_bin, model = \"K80\", pairwise.deletion = TRUE))\n\n# Function to get unique pairs of sequences with distances smaller than the threshold\nget_close_pairs &lt;- function(dist_matrix, threshold) {\n  close_pairs &lt;- which(dist_matrix &lt; threshold & dist_matrix &gt; 0 & lower.tri(dist_matrix), arr.ind = TRUE)\n  if (nrow(close_pairs) == 0) {\n    return(NULL)\n  }\n  data.frame(\n    Seq1 = rownames(dist_matrix)[close_pairs[,1]],\n    Seq2 = colnames(dist_matrix)[close_pairs[,2]],\n    Distance = dist_matrix[close_pairs]\n  )\n}\n\n# Set the threshold to 0.015488\n\nthreshold &lt;- 0.015488\n\n# Get close pairs\nclose_pairs_df &lt;- get_close_pairs(dist_matrix, threshold)\n\nif (is.null(close_pairs_df)) {\n  cat(\"No pairs found with distance smaller than\", threshold, \"\\n\")\n} else {\n  cat(\"Pairs of sequences with distances smaller than\", threshold, \":\\n\\n\")\n  \n  # Print the close pairs with original taxonomic names\n  for (i in 1:nrow(close_pairs_df)) {\n    cat(\"Pair\", i, \":\\n\")\n    cat(\"Sequence 1:\", name_mapping[close_pairs_df$Seq1[i]], \"\\n\")\n    cat(\"Sequence 2:\", name_mapping[close_pairs_df$Seq2[i]], \"\\n\")\n    cat(\"Distance:\", close_pairs_df$Distance[i], \"\\n\\n\")\n  }\n  \n  cat(\"Number of unique close pairs found:\", nrow(close_pairs_df), \"\\n\")\n}\n\n\nPairs of sequences with distances smaller than 0.015488 :\n\nPair 1 :\nSequence 1: Bacteria;Bacillota;Bacilli;Staphylococcales;Staphylococcaceae;Staphylococcus; \nSequence 2: Bacteria;Bacillota;Bacilli;Staphylococcales;Staphylococcaceae;Staphylococcus; \nDistance: 0.003067494 \n\nPair 2 :\nSequence 1: Bacteria;Actinomycetota;Actinobacteria;Kitasatosporales;Streptomycetaceae;Streptomyces; \nSequence 2: Bacteria;Actinomycetota;Actinobacteria;Kitasatosporales;Streptomycetaceae;Streptomyces; \nDistance: 0.01017183 \n\nPair 3 :\nSequence 1: Bacteria;Actinomycetota;Actinobacteria;Propionibacteriales;Propionibacteriaceae;Cutibacterium; \nSequence 2: Bacteria;Actinomycetota;Actinobacteria;Propionibacteriales;Propionibacteriaceae;Cutibacterium; \nDistance: 0.01460938 \n\nPair 4 :\nSequence 1: Bacteria;Actinomycetota;Actinobacteria;Propionibacteriales;Propionibacteriaceae;Cutibacterium; \nSequence 2: Bacteria;Actinomycetota;Actinobacteria;Propionibacteriales;Propionibacteriaceae;Cutibacterium; \nDistance: 0.01356862 \n\nPair 5 :\nSequence 1: Bacteria;Cyanobacteriota;Cyanobacteriia;Cyanobacteriales;Coleofasciculaceae;Coleofasciculus PCC-7420; \nSequence 2: Bacteria;Cyanobacteriota;Cyanobacteriia;Cyanobacteriales;Coleofasciculaceae;Coleofasciculus PCC-7420; \nDistance: 0.01137475 \n\nNumber of unique close pairs found: 5 \n\n\n\n\nCode\n# Histogram of all distances\nall_distances &lt;- dist_matrix[lower.tri(dist_matrix)]\nggplot(data.frame(Distance = all_distances), aes(x = Distance)) +\n  geom_histogram(bins = 50, fill = \"blue\", alpha = 0.7) +\n  labs(#title = \"Distribution of All Pairwise Distances (SILVA)\",\n       x = \"Kimura 2-parameter Distance\",\n       y = \"Frequency\") +\n  theme_minimal() +\n  geom_vline(xintercept = threshold, color = \"red\", linetype = \"dashed\")\n\n# Save the histogram\n# ggsave(\"silva_all_distances_histogram.png\", width = 10, height = 6)\n\n\n\n\nCode\nlibrary(ggplot2)\n\n# Function to create and save the plot\ncreate_ad_plot &lt;- function(p_values, n1, n2, filename) {\n  # Calculate 95th and 99th percentiles of the uniform distribution\n  percentile_95 &lt;- 0.95\n  percentile_99 &lt;- 0.99\n  \n  p &lt;- ggplot(data.frame(p_value = p_values), aes(x = p_value)) +\n    geom_histogram(bins = 30, fill = \"skyblue\", color = \"black\", aes(y = ..density..)) +\n    geom_hline(yintercept = 1, color = \"red\", linetype = \"dashed\") +  # Uniform distribution line\n    geom_vline(xintercept = 0.05, color = \"red\", linetype = \"dashed\") +\n    geom_vline(xintercept = percentile_95, color = \"green\", linetype = \"dashed\") +\n    geom_vline(xintercept = percentile_99, color = \"blue\", linetype = \"dashed\") +\n    scale_x_continuous(breaks = c(0, 0.05, percentile_95, percentile_99, 1),\n                       labels = c(\"0\", \"0.05\", \"95th\", \"99th\", \"1\")) +\n    labs(x = \"Anderson-Darling p-value\", \n         y = \"Density\",\n         title = paste(\"A-D p-values (n1 =\", n1, \", n2 =\", n2, \")\")) +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n  \n  ggsave(filename, p, width = 8, height = 6)\n  \n  # Print summary statistics\n  cat(\"For n1 =\", n1, \"and n2 =\", n2, \":\\n\")\n  cat(\"Proportion of p-values &lt; 0.05:\", mean(p_values &lt; 0.05), \"\\n\")\n  cat(\"Mean p-value:\", mean(p_values), \"\\n\")\n  cat(\"Median p-value:\", median(p_values), \"\\n\\n\")\n}\n\n# For n1 = 40, n2 = 160\ncreate_ad_plot(ad_results_40_160, 40, 160, \"anderson_darling_pvalues_40_160.png\")\n\n# For n1 = 100, n2 = 400\n# create_ad_plot(ad_results_100_400, 100, 400, \"anderson_darling_pvalues_100_400.png\")"
  },
  {
    "objectID": "analysis/02-coincidence-detection.html",
    "href": "analysis/02-coincidence-detection.html",
    "title": "Coincidence Detection using Distances: Simulations",
    "section": "",
    "text": "This analysis implements coincidence detection methods using genetic distances computed with the leave one out method."
  },
  {
    "objectID": "analysis/02-coincidence-detection.html#overview",
    "href": "analysis/02-coincidence-detection.html#overview",
    "title": "Coincidence Detection using Distances: Simulations",
    "section": "",
    "text": "This analysis implements coincidence detection methods using genetic distances computed with the leave one out method."
  },
  {
    "objectID": "analysis/02-coincidence-detection.html#setup",
    "href": "analysis/02-coincidence-detection.html#setup",
    "title": "Coincidence Detection using Distances: Simulations",
    "section": "Setup",
    "text": "Setup\n\n\nLoad Libraries and Helper Functions\nlibrary(Biostrings)\nlibrary(ape)\nlibrary(kSamples)\nlibrary(ggplot2)\nlibrary(gridExtra)\nset.seed(201123)  # For reproducibility\n\n\n\n\nCode\n# Read the data\naligned_seqs &lt;- readDNAStringSet(\"../data/silva_aligned_bacteria_sequences.fasta\")\ncat(\"Loaded\", length(aligned_seqs), \"sequences\\n\")\n\n\nLoaded 205 sequences"
  },
  {
    "objectID": "analysis/02-coincidence-detection.html#part-1-random-split-analysis",
    "href": "analysis/02-coincidence-detection.html#part-1-random-split-analysis",
    "title": "Coincidence Detection using Distances: Simulations",
    "section": "Part 1: Random Split Analysis",
    "text": "Part 1: Random Split Analysis\nThe original simulation with random 40/160 splits, we generate ad_distribution and make helper distance functions for the splits.\n\n\nCode\n# Function to compute within-sample minimum distances (excluding self-distances)\ncompute_within_sample_min_distances &lt;- function(dist_mat, sample_indices) {\n  sample_dist_mat &lt;- dist_mat[sample_indices, sample_indices]\n  min_distances &lt;- apply(sample_dist_mat, 1, function(row) {\n    non_zero_distances &lt;- row[row &gt; 0]  # Exclude self-distances (zeros)\n    if(length(non_zero_distances) &gt; 0) {\n      return(min(non_zero_distances))\n    } else {\n      return(NA)\n    }\n  })\n  return(min_distances[!is.na(min_distances)])  # Remove any NAs\n}\n\n# Function to compute sample-to-population minimum distances\ncompute_sample_to_population_min_distances &lt;- function(dist_mat, population_indices, sample_indices) {\n  # Distances from population to sample\n  pop_to_sample_dist_mat &lt;- dist_mat[population_indices, sample_indices]\n  min_distances &lt;- apply(pop_to_sample_dist_mat, 1, min)\n  return(min_distances)\n}\n\n\n\n\nCode\n# Main simulation function\nsimulate_ad_statistics &lt;- function(seqs, n_simulations = 500) {\n  # Compute distance matrix once\n  cat(\"Computing distance matrix...\\n\")\n  dist_matrix &lt;- as.matrix(dist.dna(as.DNAbin(seqs), model = \"K80\", pairwise.deletion = TRUE))\n  \n  ad_statistics &lt;- numeric(n_simulations)\n  \n  cat(\"Running\", n_simulations, \"simulations...\\n\")\n  \n  for(i in 1:n_simulations) {\n    if(i %% 50 == 0) cat(\"Simulation\", i, \"of\", n_simulations, \"\\n\")\n    \n    # Randomly split 200 sequences into sample (40) and population (160)\n    random_indices &lt;- sample(nrow(dist_matrix), 200)\n    sample_indices &lt;- random_indices[1:40]\n    population_indices &lt;- random_indices[41:200]\n    \n    # Compute within-sample minimum distances (n1 = up to 40, but typically 39 per point in this part)\n    within_sample_distances &lt;- compute_within_sample_min_distances(dist_matrix, sample_indices)\n    \n    # Compute sample-to-population minimum distances (n2 = 160)\n    sample_to_pop_distances &lt;- compute_sample_to_population_min_distances(dist_matrix, population_indices, sample_indices)\n    \n    # Compute Anderson-Darling statistic\n    if(length(within_sample_distances) &gt; 0 && length(sample_to_pop_distances) &gt; 0) {\n      ad_result &lt;- ad.test(within_sample_distances, sample_to_pop_distances)\n      ad_statistics[i] &lt;- ad_result$ad[1, 1]\n    } else {\n      ad_statistics[i] &lt;- NA\n    }\n  }\n  \n  # Remove any failed simulations\n  ad_statistics &lt;- ad_statistics[!is.na(ad_statistics)]\n  cat(\"Completed\", length(ad_statistics), \"successful simulations\\n\")\n  \n  return(ad_statistics)\n}\n\n\n\n\nCode\n# Run the simulations\n# This was the original one \nad_distribution &lt;- simulate_ad_statistics(aligned_seqs, n_simulations = 500)\n\n\nComputing distance matrix...\nRunning 500 simulations...\nSimulation 50 of 500 \nSimulation 100 of 500 \nSimulation 150 of 500 \nSimulation 200 of 500 \nSimulation 250 of 500 \nSimulation 300 of 500 \nSimulation 350 of 500 \nSimulation 400 of 500 \nSimulation 450 of 500 \nSimulation 500 of 500 \nCompleted 500 successful simulations"
  },
  {
    "objectID": "analysis/02-coincidence-detection.html#part-2-distance-distributions-comparison-and-ad-computation",
    "href": "analysis/02-coincidence-detection.html#part-2-distance-distributions-comparison-and-ad-computation",
    "title": "Coincidence Detection using Distances: Simulations",
    "section": "Part 2: Distance Distributions Comparison and AD computation",
    "text": "Part 2: Distance Distributions Comparison and AD computation\nHere we are using our distance between and within functions to compare the leave one out for 40 point to the “true” distance computation for 160.\n\n\nCode\n# The 4 selected simulations with density plots\n# Show within-sample vs sample-to-population distances\n# Save distance distributions for selected simulations\nsimulate_ad_statistics_with_distances &lt;- function(seqs, n_simulations = 500, save_simulations = c(150, 200, 250, 450)) {\n  # Compute distance matrix once\n  cat(\"Computing distance matrix...\\n\")\n  dist_matrix &lt;- as.matrix(dist.dna(as.DNAbin(seqs), model = \"K80\", pairwise.deletion = TRUE))\n  \n  ad_statistics &lt;- numeric(n_simulations)\n  saved_distances &lt;- list()\n  \n  cat(\"Running\", n_simulations, \"simulations...\\n\")\n  \n  for(i in 1:n_simulations) {\n    if(i %% 50 == 0) cat(\"Simulation\", i, \"of\", n_simulations, \"\\n\")\n    \n    # Randomly split 200 sequences into sample (40) and population (160)\n    random_indices &lt;- sample(nrow(dist_matrix), 200)\n    sample_indices &lt;- random_indices[1:40]\n    population_indices &lt;- random_indices[41:200]\n    \n    # Compute within-sample minimum distances (n1 = up to 40, but typically 39 per point)\n    within_sample_distances &lt;- compute_within_sample_min_distances(dist_matrix, sample_indices)\n    \n    # Compute sample-to-population minimum distances (n2 = 160)\n    sample_to_pop_distances &lt;- compute_sample_to_population_min_distances(dist_matrix, population_indices, sample_indices)\n    \n    # Save distances for selected simulations\n    if(i %in% save_simulations) {\n      saved_distances[[as.character(i)]] &lt;- list(\n        within_sample = within_sample_distances,\n        sample_to_pop = sample_to_pop_distances\n      )\n    }\n    \n    # Compute Anderson-Darling statistic\n    if(length(within_sample_distances) &gt; 0 && length(sample_to_pop_distances) &gt; 0) {\n      ad_result &lt;- ad.test(within_sample_distances, sample_to_pop_distances)\n      ad_statistics[i] &lt;- ad_result$ad[1, 1]\n    } else {\n      ad_statistics[i] &lt;- NA\n    }\n  }\n  \n  # Remove any failed simulations\n  ad_statistics &lt;- ad_statistics[!is.na(ad_statistics)]\n  cat(\"Completed\", length(ad_statistics), \"successful simulations\\n\")\n  \n  return(list(ad_statistics = ad_statistics, saved_distances = saved_distances))\n}\n\n\n\n\nCode\n# Run the simulations with distance saving\nresults &lt;- simulate_ad_statistics_with_distances(aligned_seqs, n_simulations = 500, \n                                                save_simulations = c(150, 200, 250, 450))\n\n\nComputing distance matrix...\nRunning 500 simulations...\nSimulation 50 of 500 \nSimulation 100 of 500 \nSimulation 150 of 500 \nSimulation 200 of 500 \nSimulation 250 of 500 \nSimulation 300 of 500 \nSimulation 350 of 500 \nSimulation 400 of 500 \nSimulation 450 of 500 \nSimulation 500 of 500 \nCompleted 500 successful simulations\n\n\nCode\nad_distribution &lt;- results$ad_statistics\nsaved_distances &lt;- results$saved_distances\n\n\n\nPlots comparing four of the distance pairs for Figure 7a\nMake the comparison densities and the Anderson – Darling histogram for the 500 simulations of splits.\n\n\nCode\n# Create density plots for the 4 selected simulations\nlibrary(gridExtra)\n\ndensity_plots &lt;- list()\n\nfor(i in 1:length(saved_distances)) {\n  sim_num &lt;- names(saved_distances)[i]\n  within_dist &lt;- saved_distances[[sim_num]]$within_sample\n  between_dist &lt;- saved_distances[[sim_num]]$sample_to_pop\n  \n  # Create combined data frame\n  plot_data &lt;- data.frame(\n    distance = c(within_dist, between_dist),\n    type = factor(c(rep(\"Within-Sample\", length(within_dist)), \n                    rep(\"Sample-to-Population\", length(between_dist))))\n  )\n  \n  # Calculate AD statistic for this simulation\n  ad_stat &lt;- round(ad_distribution[as.numeric(sim_num)], 3)\n  \n  density_plots[[i]] &lt;- ggplot(plot_data, aes(x = distance, fill = type)) +\n    geom_density(alpha = 0.6) +\n    scale_fill_manual(values = c(\"Within-Sample\" = \"blue\", \"Sample-to-Population\" = \"red\")) +\n    labs(#title = paste(\"Simulation\", sim_num, \"- AD =\", ad_stat),\n         x = \"Minimum Distance\",\n         y = \"Density\",\n         fill = \"Distance Type\") +\n    theme_minimal() +\n    theme(legend.position = \"none\")\n}\n\n# Arrange the 4 density plots in a 2x2 grid\ncombined_density_plot &lt;- grid.arrange(density_plots[[1]], density_plots[[2]], \n                                     density_plots[[3]], density_plots[[4]], \n                                     ncol = 2, nrow = 2)\n\n\n\n\n\nFour exemplary simulations and the two densities compared.\n\n\n\n\nCode\ncombined_density_plot\n\n\nTableGrob (2 x 2) \"arrange\": 4 grobs\n  z     cells    name           grob\n1 1 (1-1,1-1) arrange gtable[layout]\n2 2 (1-1,2-2) arrange gtable[layout]\n3 3 (2-2,1-1) arrange gtable[layout]\n4 4 (2-2,2-2) arrange gtable[layout]\n\n\nCode\n# Save the combined plot for Figure left side\n# ggsave(\"four_density_comparisons.png\", combined_density_plot, width = 4, height = 4, dpi = 300)\n\n# Summary statistics\ncat(\"\\nSummary of AD Statistics:\\n\")\n\n\n\nSummary of AD Statistics:\n\n\nCode\ncat(\"Mean:\", mean(ad_distribution), \"\\n\")\n\n\nMean: 1.545891 \n\n\nCode\ncat(\"Median:\", median(ad_distribution), \"\\n\")\n\n\nMedian: 1.21445 \n\n\nCode\ncat(\"95th Percentile:\", quantile(ad_distribution, 0.95), \"\\n\")\n\n\n95th Percentile: 3.874305 \n\n\nCode\ncat(\"Standard Deviation:\", sd(ad_distribution), \"\\n\")\n\n\nStandard Deviation: 1.085291 \n\n\nCode\n# Print AD statistics for the selected simulations\ncat(\"\\nAD Statistics for selected simulations:\\n\")\n\n\n\nAD Statistics for selected simulations:\n\n\nCode\nfor(sim_num in names(saved_distances)) {\n  cat(\"Simulation\", sim_num, \": AD =\", round(ad_distribution[as.numeric(sim_num)], 4), \"\\n\")\n}\n\n\nSimulation 150 : AD = 0.867 \nSimulation 200 : AD = 1.0111 \nSimulation 250 : AD = 2.4349 \nSimulation 450 : AD = 3.6647 \n\n\n\n\nHistogram of Anderson –Darling 500 stats for 7b\n\n\nCode\n# Create the  histogram for Figure 7b, no title \nhist_plot_nt &lt;- ggplot(data.frame(ad_statistic = ad_distribution), aes(x = ad_statistic)) +\n  geom_histogram(binwidth = 0.2, fill = \"lightblue\", color = \"darkblue\", alpha = 0.7) +\n  geom_vline(aes(xintercept = median(ad_statistic)), color = \"darkblue\", linetype = \"dashed\", size = 1) +\n  geom_vline(aes(xintercept = quantile(ad_statistic, 0.95)), color = \"purple\", linetype = \"dashed\", size = 1) +\n  labs(x = \"Anderson-Darling Statistic\",\n       y = \"Frequency\") +\n  theme_minimal() +\n  annotate(\"text\", x = median(ad_distribution)+0.6, y = 60, \n           label = paste(\"Median =\", round(median(ad_distribution), 2)), vjust = 1, color = \"darkblue\") +\n  annotate(\"text\", x = quantile(ad_distribution, 0.95), y = max(table(cut(ad_distribution, breaks = 20)))*0.1, \n           label = paste(\"95th % =\", round(quantile(ad_distribution, 0.95), 2)), vjust = -1, color = \"purple\")\n\nprint(hist_plot_nt)\n\n\n\n\n\nHistogram of the 500 Anderson–Statistic comparing the 40 LOO distances and the 160 sample to population ones.\n\n\n\n\nCode\n# ggsave(\"ADhistogram.png\",hist_plot_nt,width=5,height=4)"
  },
  {
    "objectID": "analysis/02-coincidence-detection.html#part-3-null-distribution-of-andersondarling-generation",
    "href": "analysis/02-coincidence-detection.html#part-3-null-distribution-of-andersondarling-generation",
    "title": "Coincidence Detection using Distances: Simulations",
    "section": "Part 3: Null Distribution of Anderson–Darling : Generation",
    "text": "Part 3: Null Distribution of Anderson–Darling : Generation\n\n\nCode\n# Bootstrap + mutate to create 1000-sequence population\n# Function to bootstrap and mutate sequences (your existing code)\nbootstrap_and_mutate_sequences &lt;- function(seqs, n, mutation_rate = 0.01) {\n  bootstrapped_seqs &lt;- sample(seqs, n, replace = TRUE)\n  mutated_seqs &lt;- sapply(bootstrapped_seqs, function(seq) {\n    seq_vector &lt;- strsplit(as.character(seq), \"\")[[1]]\n    mutation_sites &lt;- which(runif(length(seq_vector)) &lt; mutation_rate)\n    seq_vector[mutation_sites] &lt;- sample(c(\"A\", \"C\", \"G\", \"T\"), length(mutation_sites), replace = TRUE)\n    paste(seq_vector, collapse = \"\")\n  })\n  return(DNAStringSet(mutated_seqs))\n}\n\n\n\nCreate metapopulation\n\n\nCode\npopulation_size &lt;- 1000\ncat(\"Creating metapopulation of\", population_size, \"sequences...\\n\")\n\n\nCreating metapopulation of 1000 sequences...\n\n\nCode\npopulation_seqs &lt;- bootstrap_and_mutate_sequences(aligned_seqs, population_size, mutation_rate = 0.01)\n\n\n\n\nGenerate clean_null_ad_distribution with independent samples\nNull from 40 random to 40 random and 160 random to 40 random\n\n\nCode\n# Function to generate the clean null distribution (all independent random sets)\ngenerate_clean_null_ad_distribution &lt;- function(metapopulation_seqs, n_simulations = 500) {\n  # Compute distance matrix for the entire metapopulation once\n  cat(\"Computing distance matrix for metapopulation...\\n\")\n  dist_matrix &lt;- as.matrix(dist.dna(as.DNAbin(metapopulation_seqs), model = \"K80\", pairwise.deletion = TRUE))\n  \n  clean_null_ad_statistics &lt;- numeric(n_simulations)\n  \n  cat(\"Generating clean null distribution with\", n_simulations, \"simulations...\\n\")\n  \n  for(i in 1:n_simulations) {\n    if(i %% 50 == 0) cat(\"Clean null simulation\", i, \"of\", n_simulations, \"\\n\")\n    \n    # Draw four independent random sets from metapopulation\n    # Set 1: 40 random points (group 1)\n    random_40_group1 &lt;- sample(nrow(dist_matrix), 40)\n    \n    # Set 2: 40 different random points (target for group 1)\n    remaining_indices &lt;- setdiff(1:nrow(dist_matrix), random_40_group1)\n    random_40_target1 &lt;- sample(remaining_indices, 40)\n    \n    # Set 3: 160 different random points (group 2)\n    remaining_indices_2 &lt;- setdiff(remaining_indices, random_40_target1)\n    random_160_group2 &lt;- sample(remaining_indices_2, 160)\n    \n    # Set 4: 40 fresh different random points (target for group 2)\n    remaining_indices_3 &lt;- setdiff(remaining_indices_2, random_160_group2)\n    random_40_target2 &lt;- sample(remaining_indices_3, 40)\n    \n    # Compute distances: 40 random → 40 random target (set 1)\n    group1_to_target_distances &lt;- compute_sample_to_population_min_distances(\n      dist_matrix, random_40_group1, random_40_target1)\n    \n    # Compute distances: 160 random → different 40 random target (set 2)\n    group2_to_target_distances &lt;- compute_sample_to_population_min_distances(\n      dist_matrix, random_160_group2, random_40_target2)\n    \n    # Compute Anderson-Darling statistic\n    if(length(group1_to_target_distances) &gt; 0 && length(group2_to_target_distances) &gt; 0) {\n      ad_result &lt;- ad.test(group1_to_target_distances, group2_to_target_distances)\n      clean_null_ad_statistics[i] &lt;- ad_result$ad[1, 1]\n    } else {\n      clean_null_ad_statistics[i] &lt;- NA\n    }\n  }\n  \n  # Remove any failed simulations\n  clean_null_ad_statistics &lt;- clean_null_ad_statistics[!is.na(clean_null_ad_statistics)]\n  cat(\"Completed\", length(clean_null_ad_statistics), \"successful clean null simulations\\n\")\n  \n  return(clean_null_ad_statistics)\n}\n\n# Generate the clean null distribution\nclean_null_ad_distribution &lt;- generate_clean_null_ad_distribution(population_seqs, n_simulations = 500)\n\n\nComputing distance matrix for metapopulation...\nGenerating clean null distribution with 500 simulations...\nClean null simulation 50 of 500 \nClean null simulation 100 of 500 \nClean null simulation 150 of 500 \nClean null simulation 200 of 500 \nClean null simulation 250 of 500 \nClean null simulation 300 of 500 \nClean null simulation 350 of 500 \nClean null simulation 400 of 500 \nClean null simulation 450 of 500 \nClean null simulation 500 of 500 \nCompleted 500 successful clean null simulations\n\n\nCode\n# Compare clean null vs original data split\ncomparison_data &lt;- data.frame(\n  AD_Statistic = c(ad_distribution, clean_null_ad_distribution),\n  Distribution = factor(c(rep(\"Leave one out split 200\", length(ad_distribution)),\n                          rep(\"Null all independent\", length(clean_null_ad_distribution))))\n)\n\n\n\n\nCompare the densities of the 500 random splits AD to the null generated above\nThis is the plot we show in the Figure.\n\n\nCode\n# Create comparison plot\ncomparison_plot &lt;- ggplot(comparison_data, aes(x = AD_Statistic, fill = Distribution)) +\n  geom_density(alpha = 0.6) +\n  scale_fill_manual(values = c(\"Leave one out split 200\" = \"blue\", \n                              \"Null all independent\" = \"red\")) +\n  labs(#title = \"Clean Comparison: Original Data Split vs Fully Independent Random Null\",\n       x = \"Anderson-Darling Statistic\",\n       y = \"Density\",\n       fill = \"Distribution Type\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\nprint(comparison_plot)\n\n\n\n\n\nComparison: Original Data Split vs Fully Independent Random Null\n\n\n\n\nCode\n# Summary statistics comparison\ncat(\"\\nClean Comparison (Fully Independent):\\n\")\n\n\n\nClean Comparison (Fully Independent):\n\n\nCode\ncat(\"Original Data Split - Mean:\", round(mean(ad_distribution), 4), \n    \", Median:\", round(median(ad_distribution), 4), \n    \", 95th percentile:\", round(quantile(ad_distribution, 0.95), 4), \"\\n\")\n\n\nOriginal Data Split - Mean: 1.5459 , Median: 1.2145 , 95th percentile: 3.8743 \n\n\nCode\ncat(\"Clean Null - Mean:\", round(mean(clean_null_ad_distribution), 4), \n    \", Median:\", round(median(clean_null_ad_distribution), 4), \n    \", 95th percentile:\", round(quantile(clean_null_ad_distribution, 0.95), 4), \"\\n\")\n\n\nClean Null - Mean: 1.4584 , Median: 1.0966 , 95th percentile: 3.7428 \n\n\nCode\n# Calculate p-value\nclean_null_95th &lt;- quantile(clean_null_ad_distribution, 0.95)\nproportion_exceeding_clean_null &lt;- mean(ad_distribution &gt; clean_null_95th)\n\ncat(\"Proportion of original AD statistics exceeding clean null 95th percentile:\", \n    round(proportion_exceeding_clean_null, 4), \"\\n\")\n\n\nProportion of original AD statistics exceeding clean null 95th percentile: 0.054 \n\n\nCode\n# This gives you a clean p-value for testing population structure\ncat(\"Estimated p-value for test:\", round(proportion_exceeding_clean_null, 4), \"\\n\")\n\n\nEstimated p-value for test: 0.054 \n\n\nCode\n# Save the comparison plot without the legend\n# ggsave(\"ad_fully_independent_comparison.png\", comparison_plot_nt, width = 6, height = 4, dpi = 300)"
  },
  {
    "objectID": "analysis/02-coincidence-detection.html#part-4-final-comparison-through-a-quantile-quantile-plot",
    "href": "analysis/02-coincidence-detection.html#part-4-final-comparison-through-a-quantile-quantile-plot",
    "title": "Coincidence Detection using Distances: Simulations",
    "section": "Part 4: Final Comparison through a quantile-quantile plot",
    "text": "Part 4: Final Comparison through a quantile-quantile plot\n\n\nCode\n# Compare ad_distribution vs clean_null_ad_distribution\n\n# Create QQ plot data\nqq_data &lt;- data.frame(\n  clean_null = sort(clean_null_ad_distribution),\n  original_split = sort(ad_distribution)\n)\n\n# Create QQ plot\nqq_plot &lt;- ggplot(qq_data, aes(x = clean_null, y = original_split)) +\n  geom_point(alpha = 0.6) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\",linewidth=1.5) +\n  labs(x = \"Null Distribution AD Quantiles\",\n       y = \"Data Split AD Quantiles\") +\n  theme_minimal()\n\nprint(qq_plot)\n\n\n\n\n\nQQ plot for AD with null on x axis and data split AD on y.\n\n\n\n\nCode\n# Save the QQ plot\n# ggsave(\"qq_plot_null_vs_original.png\", qq_plot, width = 4, height = 4, dpi = 300)"
  },
  {
    "objectID": "analysis/04-prediction-sets-regression.html",
    "href": "analysis/04-prediction-sets-regression.html",
    "title": "Prediction set coverage on a real example",
    "section": "",
    "text": "Code\nlibrary(bootstrap)  # For the cholost dataset\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(gridExtra)\nset.seed(20050724)\n\n# Load the cholost dataset\ndata(cholost)\n\n# Confirm dataset structure\nhead(cholost)\n\n\n   z     y\n1  0 -5.25\n2 27 -1.50\n3 71 59.50\n4 95 32.50\n5  0 -7.25\n6 28 23.50\n\n\nCode\ndim(cholost)  # Should be 164×2\n\n\n[1] 164   2"
  },
  {
    "objectID": "analysis/04-prediction-sets-regression.html#using-the-cholostyramine-dataset-from-the-bootstrap-package",
    "href": "analysis/04-prediction-sets-regression.html#using-the-cholostyramine-dataset-from-the-bootstrap-package",
    "title": "Prediction set coverage on a real example",
    "section": "",
    "text": "Code\nlibrary(bootstrap)  # For the cholost dataset\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(gridExtra)\nset.seed(20050724)\n\n# Load the cholost dataset\ndata(cholost)\n\n# Confirm dataset structure\nhead(cholost)\n\n\n   z     y\n1  0 -5.25\n2 27 -1.50\n3 71 59.50\n4 95 32.50\n5  0 -7.25\n6 28 23.50\n\n\nCode\ndim(cholost)  # Should be 164×2\n\n\n[1] 164   2"
  },
  {
    "objectID": "analysis/04-prediction-sets-regression.html#functions-to-compute-the-prediction-interval",
    "href": "analysis/04-prediction-sets-regression.html#functions-to-compute-the-prediction-interval",
    "title": "Prediction set coverage on a real example",
    "section": "Functions to compute the prediction interval",
    "text": "Functions to compute the prediction interval\n\n\nCode\n# Function to calculate OLS prediction interval\ncalculate_pred_interval &lt;- function(X, Y, X_new, alpha = 0.05) {\n  n &lt;- nrow(X)\n  p &lt;- ncol(X)\n  \n  # Calculate beta hat\n  beta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% Y\n  \n  # Calculate fitted values\n  Y_hat &lt;- X %*% beta_hat\n  \n  # Calculate sigma hat squared\n  sigma_hat_squared &lt;- sum((Y - Y_hat)^2) / (n - p)\n  sigma_hat &lt;- sqrt(sigma_hat_squared)\n  \n  # Make sure X_new is properly formatted as a row vector\n  if(is.vector(X_new)) X_new &lt;- matrix(X_new, nrow=1)\n  \n  # Calculate prediction for new observation\n  Y_pred &lt;- as.numeric(X_new %*% beta_hat)\n  \n  # Calculate prediction interval width\n  pred_var &lt;- sigma_hat_squared * (1 + as.numeric(X_new %*% solve(t(X) %*% X) %*% t(X_new)))\n  pred_interval_width &lt;- qnorm(1 - alpha/2) * sqrt(pred_var)\n  \n  # Return prediction and interval bounds\n  return(list(\n    prediction = Y_pred,\n    lower = Y_pred - pred_interval_width,\n    upper = Y_pred + pred_interval_width\n  ))\n}\n\n\n\n\nCode\n# Function for leave-one-out estimate of coverage probability\nloo_coverage_probability &lt;- function(X, Y, alpha = 0.05) {\n  n &lt;- nrow(X)\n  covered &lt;- 0\n  \n  for (i in 1:n) {\n    X_train &lt;- X[-i, , drop = FALSE]\n    Y_train &lt;- Y[-i]\n    X_test &lt;- matrix(X[i, ], nrow = 1)\n    Y_test &lt;- Y[i]\n    \n    tryCatch({\n      pred_interval &lt;- calculate_pred_interval(X_train, Y_train, X_test, alpha)\n      if (Y_test &gt;= pred_interval$lower && Y_test &lt;= pred_interval$upper) {\n        covered &lt;- covered + 1\n      }\n    }, error = function(e) {\n      cat(\"Error in LOO iteration\", i, \":\", e$message, \"\\n\")\n    })\n  }\n  \n  return(covered / n)\n}\n\n\n\n\nCode\n# Prepare the dataset - handling missing values if any\ncholost_clean &lt;- na.omit(cholost)\n\n# Define response and predictor\nresponse_var &lt;- \"y\"  # The response variable\npredictor_var &lt;- \"z\"  # The predictor variable\n\n# Repeat the experiment multiple times to get a distribution of results\nn_reps &lt;- 1000\nresults &lt;- data.frame(\n  rep = 1:n_reps,\n  true_coverage = numeric(n_reps),\n  loo_coverage = numeric(n_reps),\n  diff_squared = numeric(n_reps)\n)\n\nfor (rep in 1:n_reps) {\n  # Randomly split the data into training (50%) and test (50%) sets\n  n_total &lt;- nrow(cholost_clean)\n  train_indices &lt;- sample(1:n_total, size = floor(n_total/2))\n  \n  train_data &lt;- cholost_clean[train_indices, ]\n  test_data &lt;- cholost_clean[-train_indices, ]\n  \n  # Prepare matrices for training\n  X_train &lt;- as.matrix(train_data[, predictor_var, drop = FALSE])\n  Y_train &lt;- train_data[[response_var]]\n  \n  # Prepare matrices for testing\n  X_test &lt;- as.matrix(test_data[, predictor_var, drop = FALSE])\n  Y_test &lt;- test_data[[response_var]]\n  \n  # Calculate true coverage on test set\n  covered &lt;- 0\n  n_test &lt;- nrow(test_data)\n  \n  for (i in 1:n_test) {\n    tryCatch({\n      pred_interval &lt;- calculate_pred_interval(X_train, Y_train, X_test[i, , drop = FALSE], alpha = 0.05)\n      if (Y_test[i] &gt;= pred_interval$lower && Y_test[i] &lt;= pred_interval$upper) {\n        covered &lt;- covered + 1\n      }\n    }, error = function(e) {\n      cat(\"Error in test coverage calculation, iteration\", i, \":\", e$message, \"\\n\")\n    })\n  }\n  \n  true_cov &lt;- covered / n_test\n  \n  # Calculate LOO coverage estimate on training set\n  loo_cov &lt;- loo_coverage_probability(X_train, Y_train, alpha = 0.05)\n  \n  # Store results\n  results[rep, ] &lt;- c(rep, true_cov, loo_cov, (true_cov - loo_cov)^2)\n}\n\n# Calculate average results\navg_results &lt;- summarize(results,\n                      mean_true_coverage = mean(true_coverage),\n                      mean_loo_coverage = mean(loo_coverage),\n                      sd_true_coverage = sd(true_coverage),\n                      sd_loo_coverage = sd(loo_coverage),\n                      mean_squared_diff = mean(diff_squared))\n\n# Create visualization\np1 &lt;- ggplot(results, aes(x = loo_coverage, y = true_coverage)) +\n  geom_point(alpha = 0.6) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +\n  labs(title = \"LOO Coverage Estimate vs. True Coverage\",\n       subtitle = paste(\"Cholost Dataset (\", n_reps, \"random splits)\"),\n       x = \"Leave-One-Out Coverage Estimate\",\n       y = \"True Coverage on Test Set\") +\n  theme_minimal()\n\n# Histogram of differences\nresults$difference &lt;- results$true_coverage - results$loo_coverage\np2 &lt;- ggplot(results, aes(x = difference)) +\n  geom_histogram(bins = 10, fill = \"steelblue\", color = \"black\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(#title = \"Distribution of Differences\",\n       #subtitle = #\"True Coverage - LOO Coverage Estimate\",\n       x = \"Difference\",\n       y = \"Count\") +\n  theme_minimal()\n\np2\n\n\n\n\n\n\n\n\n\nCode\nggsave(\"histogramdifferences.png\",p2,width=6,height=4,bg=\"white\")\n\n\n# Create a scatter plot with the data and prediction intervals\n# (Using one random training set for visualization)\nset.seed(44112)\ntrain_indices &lt;- sample(1:nrow(cholost_clean), size = floor(nrow(cholost_clean)/2))\ntrain_data &lt;- cholost_clean[train_indices, ]\ntest_data &lt;- cholost_clean[-train_indices, ]\n\nX_train &lt;- as.matrix(train_data[, predictor_var, drop = FALSE])\nY_train &lt;- train_data[[response_var]]\n\n# Create a grid of values for z to show the prediction interval\nz_grid &lt;- seq(min(cholost_clean$z), max(cholost_clean$z), length.out = 100)\nX_grid &lt;- matrix(z_grid, ncol = 1)\npredictions &lt;- data.frame(z = z_grid, y_pred = NA, lower = NA, upper = NA)\n\nfor (i in 1:length(z_grid)) {\n  pred_interval &lt;- calculate_pred_interval(X_train, Y_train, X_grid[i, , drop = FALSE], alpha = 0.05)\n  predictions$y_pred[i] &lt;- pred_interval$prediction\n  predictions$lower[i] &lt;- pred_interval$lower\n  predictions$upper[i] &lt;- pred_interval$upper\n}\n\np3 &lt;- ggplot() +\n  geom_point(data = train_data, aes(x = z, y = y, color = \"Training Data\")) +\n  geom_point(data = test_data, aes(x = z, y = y, color = \"Test Data\")) +\n  geom_line(data = predictions, aes(x = z, y = y_pred), color = \"blue\") +\n  geom_ribbon(data = predictions, aes(x = z, ymin = lower, ymax = upper), \n              alpha = 0.2, fill = \"blue\") +\n  scale_color_manual(values = c(\"Training Data\" = \"black\", \"Test Data\" = \"red\")) +\n  labs(title = \"Cholost Dataset with Prediction Intervals\",\n       subtitle = paste(\"True Coverage:\", round(mean(results$true_coverage), 4), \n                        \"| LOO Estimate:\", round(mean(results$loo_coverage), 4)),\n        x = \"Compliance\", y = \"Improvement\", color = \"Data Type\") +\n  theme_minimal()\n\n# Display plots\n#grid.arrange(p1, p2, p3, ncol = 2, layout_matrix = rbind(c(1,2), c(3,3)))\n\nprint(p1)\n\n\n\n\n\n\n\n\n\nCode\nprint(p2)\n\n\n\n\n\n\n\n\n\nCode\nprint(p3)\n\n\n\n\n\n\n\n\n\nCode\n# Print summary statistics\ncat(\"\\n--- Summary Statistics ---\\n\")\n\n\n\n--- Summary Statistics ---\n\n\nCode\nprint(avg_results)\n\n\n  mean_true_coverage mean_loo_coverage sd_true_coverage sd_loo_coverage\n1          0.9335976         0.9334878       0.03016683       0.0139323\n  mean_squared_diff\n1       0.001547739\n\n\nCode\ncat(\"\\nMean True Coverage:\", round(avg_results$mean_true_coverage, 4))\n\n\n\nMean True Coverage: 0.9336\n\n\nCode\ncat(\"\\nMean LOO Coverage Estimate:\", round(avg_results$mean_loo_coverage, 4))\n\n\n\nMean LOO Coverage Estimate: 0.9335\n\n\nCode\ncat(\"\\nMean Squared Difference:\", round(avg_results$mean_squared_diff, 6))\n\n\n\nMean Squared Difference: 0.001548\n\n\nCode\ncat(\"\\nStandard Deviation of True Coverage:\", round(avg_results$sd_true_coverage, 4))\n\n\n\nStandard Deviation of True Coverage: 0.0302\n\n\nCode\ncat(\"\\nStandard Deviation of LOO Coverage:\", round(avg_results$sd_loo_coverage, 4))\n\n\n\nStandard Deviation of LOO Coverage: 0.0139\n\n\nCode\n# Also fit a standard linear model once to examine the relationship\ntrain_lm &lt;- lm(y ~ z, data = train_data)\nsummary(train_lm)\n\n\n\nCall:\nlm(formula = y ~ z, data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-56.387 -12.532   0.995  16.522  39.871 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.35902    4.84932   0.074    0.941    \nz            0.55122    0.07059   7.809 1.91e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.38 on 80 degrees of freedom\nMultiple R-squared:  0.4326,    Adjusted R-squared:  0.4255 \nF-statistic: 60.98 on 1 and 80 DF,  p-value: 1.91e-11"
  },
  {
    "objectID": "analysis/04-prediction-sets-regression.html#the-quadratic-version",
    "href": "analysis/04-prediction-sets-regression.html#the-quadratic-version",
    "title": "Prediction set coverage on a real example",
    "section": "The quadratic version",
    "text": "The quadratic version\n\n\nCode\nlibrary(bootstrap)  # For the cholost dataset\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(gridExtra)\nset.seed(42)\n\n# Load the cholost dataset\ndata(cholost)\n\n# Confirm dataset structure\nhead(cholost)\n\n\n   z     y\n1  0 -5.25\n2 27 -1.50\n3 71 59.50\n4 95 32.50\n5  0 -7.25\n6 28 23.50\n\n\nCode\ndim(cholost)  # Should be 164×2\n\n\n[1] 164   2\n\n\nCode\n# Function to calculate quadratic regression prediction interval\ncalculate_quad_pred_interval &lt;- function(X, Y, X_new, alpha = 0.05) {\n  # For quadratic regression, we'll add a squared term to the design matrix\n  X_quad &lt;- cbind(X, X^2)\n  colnames(X_quad) &lt;- c(\"z\", \"z_squared\")\n  \n  # For prediction, also square the new X\n  X_new_quad &lt;- cbind(X_new, X_new^2)\n  \n  n &lt;- nrow(X_quad)\n  p &lt;- ncol(X_quad)\n  \n  # Calculate beta hat\n  beta_hat &lt;- solve(t(X_quad) %*% X_quad) %*% t(X_quad) %*% Y\n  \n  # Calculate fitted values\n  Y_hat &lt;- X_quad %*% beta_hat\n  \n  # Calculate sigma hat squared\n  sigma_hat_squared &lt;- sum((Y - Y_hat)^2) / (n - p)\n  sigma_hat &lt;- sqrt(sigma_hat_squared)\n  \n  # Calculate prediction for new observation\n  Y_pred &lt;- as.numeric(X_new_quad %*% beta_hat)\n  \n  # Calculate prediction interval width\n  pred_var &lt;- sigma_hat_squared * (1 + as.numeric(X_new_quad %*% solve(t(X_quad) %*% X_quad) %*% t(X_new_quad)))\n  pred_interval_width &lt;- qnorm(1 - alpha/2) * sqrt(pred_var)\n  \n  # Return prediction and interval bounds\n  return(list(\n    prediction = Y_pred,\n    lower = Y_pred - pred_interval_width,\n    upper = Y_pred + pred_interval_width\n  ))\n}\n\n# Function for leave-one-out estimate of coverage probability with quadratic regression\nloo_coverage_probability &lt;- function(X, Y, alpha = 0.05) {\n  n &lt;- nrow(X)\n  covered &lt;- 0\n  \n  for (i in 1:n) {\n    X_train &lt;- X[-i, , drop = FALSE]\n    Y_train &lt;- Y[-i]\n    X_test &lt;- matrix(X[i, ], nrow = 1)\n    Y_test &lt;- Y[i]\n    \n    tryCatch({\n      pred_interval &lt;- calculate_quad_pred_interval(X_train, Y_train, X_test, alpha)\n      if (Y_test &gt;= pred_interval$lower && Y_test &lt;= pred_interval$upper) {\n        covered &lt;- covered + 1\n      }\n    }, error = function(e) {\n      cat(\"Error in LOO iteration\", i, \":\", e$message, \"\\n\")\n    })\n  }\n  \n  return(covered / n)\n}\n\n# Prepare the dataset - handling missing values if any\ncholost_clean &lt;- na.omit(cholost)\n\n# Define response and predictor\nresponse_var &lt;- \"y\"  # The response variable\npredictor_var &lt;- \"z\"  # The predictor variable\n\n# Repeat the experiment multiple times to get a distribution of results\nn_reps &lt;- 1000\nresults &lt;- data.frame(\n  rep = 1:n_reps,\n  true_coverage = numeric(n_reps),\n  loo_coverage = numeric(n_reps),\n  diff_squared = numeric(n_reps)\n)\n\nfor (rep in 1:n_reps) {\n  # Randomly split the data into training (50%) and test (50%) sets\n  n_total &lt;- nrow(cholost_clean)\n  train_indices &lt;- sample(1:n_total, size = floor(n_total/2))\n  \n  train_data &lt;- cholost_clean[train_indices, ]\n  test_data &lt;- cholost_clean[-train_indices, ]\n  \n  # Prepare matrices for training\n  X_train &lt;- as.matrix(train_data[, predictor_var, drop = FALSE])\n  Y_train &lt;- train_data[[response_var]]\n  \n  # Prepare matrices for testing\n  X_test &lt;- as.matrix(test_data[, predictor_var, drop = FALSE])\n  Y_test &lt;- test_data[[response_var]]\n  \n  # Calculate true coverage on test set\n  covered &lt;- 0\n  n_test &lt;- nrow(test_data)\n  \n  for (i in 1:n_test) {\n    tryCatch({\n      pred_interval &lt;- calculate_quad_pred_interval(X_train, Y_train, X_test[i, , drop = FALSE], alpha = 0.05)\n      if (Y_test[i] &gt;= pred_interval$lower && Y_test[i] &lt;= pred_interval$upper) {\n        covered &lt;- covered + 1\n      }\n    }, error = function(e) {\n      cat(\"Error in test coverage calculation, iteration\", i, \":\", e$message, \"\\n\")\n    })\n  }\n  \n  true_cov &lt;- covered / n_test\n  \n  # Calculate LOO coverage estimate on training set\n  loo_cov &lt;- loo_coverage_probability(X_train, Y_train, alpha = 0.05)\n  \n  # Store results\n  results[rep, ] &lt;- c(rep, true_cov, loo_cov, (true_cov - loo_cov)^2)\n}\n\n# Calculate average results\navg_results &lt;- summarize(results,\n                      mean_true_coverage = mean(true_coverage),\n                      mean_loo_coverage = mean(loo_coverage),\n                      sd_true_coverage = sd(true_coverage),\n                      sd_loo_coverage = sd(loo_coverage),\n                      mean_squared_diff = mean(diff_squared))\n\n# Create visualization\np1 &lt;- ggplot(results, aes(x = loo_coverage, y = true_coverage)) +\n  geom_point(alpha = 0.6) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +\n  labs(title = \"LOO Coverage Estimate vs. True Coverage\",\n       subtitle = paste(\"Cholost Dataset - Quadratic Model (\", n_reps, \"random splits)\"),\n       x = \"Leave-One-Out Coverage Estimate\",\n       y = \"True Coverage on Test Set\") +\n  theme_minimal()\n\n# Histogram of differences\nresults$difference &lt;- results$true_coverage - results$loo_coverage\np2 &lt;- ggplot(results, aes(x = difference)) +\n  geom_histogram(bins = 10, fill = \"steelblue\", color = \"black\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Distribution of Differences\",\n       subtitle = \"True Coverage - LOO Coverage Estimate\",\n       x = \"Difference\",\n       y = \"Count\") +\n  theme_minimal()\n\n# Create a scatter plot with the data and prediction intervals\n# (Using one random training set for visualization)\nset.seed(123)\ntrain_indices &lt;- sample(1:nrow(cholost_clean), size = floor(nrow(cholost_clean)/2))\ntrain_data &lt;- cholost_clean[train_indices, ]\ntest_data &lt;- cholost_clean[-train_indices, ]\n\nX_train &lt;- as.matrix(train_data[, predictor_var, drop = FALSE])\nY_train &lt;- train_data[[response_var]]\n\n# Create a grid of values for z to show the prediction interval\nz_grid &lt;- seq(min(cholost_clean$z), max(cholost_clean$z), length.out = 100)\nX_grid &lt;- matrix(z_grid, ncol = 1)\npredictions &lt;- data.frame(z = z_grid, y_pred = NA, lower = NA, upper = NA)\n\nfor (i in 1:length(z_grid)) {\n  pred_interval &lt;- calculate_quad_pred_interval(X_train, Y_train, X_grid[i, , drop = FALSE], alpha = 0.05)\n  predictions$y_pred[i] &lt;- pred_interval$prediction\n  predictions$lower[i] &lt;- pred_interval$lower\n  predictions$upper[i] &lt;- pred_interval$upper\n}\n\np3 &lt;- ggplot() +\n  geom_point(data = train_data, aes(x = z, y = y, color = \"Training Data\")) +\n  geom_point(data = test_data, aes(x = z, y = y, color = \"Test Data\")) +\n  geom_line(data = predictions, aes(x = z, y = y_pred), color = \"blue\") +\n  geom_ribbon(data = predictions, aes(x = z, ymin = lower, ymax = upper), \n              alpha = 0.2, fill = \"blue\") +\n  scale_color_manual(values = c(\"Training Data\" = \"black\", \"Test Data\" = \"red\")) +\n  labs(title = \"Cholost Dataset with Quadratic Prediction Intervals\",\n       subtitle = paste(\"True Coverage:\", round(mean(results$true_coverage), 4), \n                        \"| LOO Estimate:\", round(mean(results$loo_coverage), 4)),\n       x = \"Compliance\", y = \"Improvement\", color = \"Data Type\") +\n  theme_minimal()\n\n# Display plots\n#grid.arrange(p1, p2, p3, ncol = 2, layout_matrix = rbind(c(1,2), c(3,3)))\n\nprint(p1)\n\n\n\n\n\n\n\n\n\nCode\nprint(p2)\n\n\n\n\n\n\n\n\n\nCode\nprint(p3)\n\n\n\n\n\n\n\n\n\nCode\n# Print summary statistics\ncat(\"\\n--- Summary Statistics ---\\n\")\n\n\n\n--- Summary Statistics ---\n\n\nCode\nprint(avg_results)\n\n\n  mean_true_coverage mean_loo_coverage sd_true_coverage sd_loo_coverage\n1          0.9358902         0.9344268       0.03171133       0.0135256\n  mean_squared_diff\n1        0.00148602\n\n\nCode\ncat(\"\\nMean True Coverage:\", round(avg_results$mean_true_coverage, 4))\n\n\n\nMean True Coverage: 0.9359\n\n\nCode\ncat(\"\\nMean LOO Coverage Estimate:\", round(avg_results$mean_loo_coverage, 4))\n\n\n\nMean LOO Coverage Estimate: 0.9344\n\n\nCode\ncat(\"\\nMean Squared Difference:\", round(avg_results$mean_squared_diff, 6))\n\n\n\nMean Squared Difference: 0.001486\n\n\nCode\ncat(\"\\nStandard Deviation of True Coverage:\", round(avg_results$sd_true_coverage, 4))\n\n\n\nStandard Deviation of True Coverage: 0.0317\n\n\nCode\ncat(\"\\nStandard Deviation of LOO Coverage:\", round(avg_results$sd_loo_coverage, 4))\n\n\n\nStandard Deviation of LOO Coverage: 0.0135\n\n\nCode\n# Also fit a standard quadratic model once to examine the relationship\ntrain_quad &lt;- lm(y ~ z + I(z^2), data = train_data)\nsummary(train_quad)\n\n\n\nCall:\nlm(formula = y ~ z + I(z^2), data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-53.568 -13.064   3.445  17.227  55.313 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  5.458842   8.232416   0.663    0.509  \nz           -0.019853   0.364572  -0.054    0.957  \nI(z^2)       0.006024   0.003207   1.878    0.064 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22.64 on 79 degrees of freedom\nMultiple R-squared:  0.4941,    Adjusted R-squared:  0.4813 \nF-statistic: 38.58 on 2 and 79 DF,  p-value: 2.046e-12"
  },
  {
    "objectID": "analysis/01-convex-hulls.html",
    "href": "analysis/01-convex-hulls.html",
    "title": "Convex Hull Theorem Illustration",
    "section": "",
    "text": "Code\n# Load required libraries\nlibrary(ggplot2)\nlibrary(grDevices)\n\n# Set seed for reproducibility\nset.seed(50720235)\n\n# Generate random uniform points in rectangle [0,4] by [0,2]\nn_points &lt;- 100  # Number of random points\nx &lt;- runif(n_points, min = 0, max = 4)\ny &lt;- runif(n_points, min = 0, max = 2)\n\n# Create data frame\npoints_df &lt;- data.frame(x = x, y = y)\n\n# Compute convex hull\nhull_indices &lt;- chull(x, y)\nhull_points &lt;- points_df[hull_indices, ]\n\n# Add first point at the end to close the polygon\nhull_points &lt;- rbind(hull_points, hull_points[1, ])\n\n# Count extreme points (vertices of convex hull)\nn_extreme_points &lt;- length(hull_indices)\n\n# Create the plot\np &lt;- ggplot() +\n  # Add shaded convex hull region in light blue\n  geom_polygon(data = hull_points, \n               aes(x = x, y = y), \n               fill = \"lightblue\", \n               alpha = 0.6, \n               color = \"blue\", \n               linewidth = 1) +\n  \n  # Add all random points in gray\n  geom_point(data = points_df, \n             aes(x = x, y = y), \n             color = \"gray50\", \n             size = 2) +\n  \n  # Add extreme points (hull vertices) in red\n  geom_point(data = points_df[hull_indices, ], \n             aes(x = x, y = y), \n             color = \"red\", \n             size = 3) +\n  \n  # Formatting\n  xlim(0, 4) +\n  ylim(0, 2) +\n  labs(title = paste(\"Convex Hull of\", n_points, \"Random Points\"),\n       subtitle = paste(\"Number of extreme points:\", n_extreme_points),\n       x = \"X coordinate\", \n       y = \"Y coordinate\") +\n  theme_minimal() +\n  theme(panel.grid.minor = element_blank(),\n        plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5)) +\n  coord_fixed()  # Maintain aspect ratio\n\n# Display the plot\nprint(p)\n\n\n\n\n\n\n\n\n\nCode\n# Print summary information\ncat(\"Summary:\\n\")\n\n\nSummary:\n\n\nCode\ncat(\"Total points generated:\", n_points, \"\\n\")\n\n\nTotal points generated: 100 \n\n\nCode\ncat(\"Number of extreme points (convex hull vertices):\", n_extreme_points, \"\\n\")\n\n\nNumber of extreme points (convex hull vertices): 15 \n\n\nCode\ncat(\"Proportion of extreme points:\", round(n_extreme_points/n_points, 3), \"\\n\")\n\n\nProportion of extreme points: 0.15 \n\n\nCode\n# Save the plot file (uncomment as necessary)\n# ggsave(\"convex_hull_plot.png\", plot = p, width = 8, height = 6, units = \"in\")"
  },
  {
    "objectID": "analysis/01-convex-hulls.html#a-convex-hull-with-100-points-figure-1",
    "href": "analysis/01-convex-hulls.html#a-convex-hull-with-100-points-figure-1",
    "title": "Convex Hull Theorem Illustration",
    "section": "",
    "text": "Code\n# Load required libraries\nlibrary(ggplot2)\nlibrary(grDevices)\n\n# Set seed for reproducibility\nset.seed(50720235)\n\n# Generate random uniform points in rectangle [0,4] by [0,2]\nn_points &lt;- 100  # Number of random points\nx &lt;- runif(n_points, min = 0, max = 4)\ny &lt;- runif(n_points, min = 0, max = 2)\n\n# Create data frame\npoints_df &lt;- data.frame(x = x, y = y)\n\n# Compute convex hull\nhull_indices &lt;- chull(x, y)\nhull_points &lt;- points_df[hull_indices, ]\n\n# Add first point at the end to close the polygon\nhull_points &lt;- rbind(hull_points, hull_points[1, ])\n\n# Count extreme points (vertices of convex hull)\nn_extreme_points &lt;- length(hull_indices)\n\n# Create the plot\np &lt;- ggplot() +\n  # Add shaded convex hull region in light blue\n  geom_polygon(data = hull_points, \n               aes(x = x, y = y), \n               fill = \"lightblue\", \n               alpha = 0.6, \n               color = \"blue\", \n               linewidth = 1) +\n  \n  # Add all random points in gray\n  geom_point(data = points_df, \n             aes(x = x, y = y), \n             color = \"gray50\", \n             size = 2) +\n  \n  # Add extreme points (hull vertices) in red\n  geom_point(data = points_df[hull_indices, ], \n             aes(x = x, y = y), \n             color = \"red\", \n             size = 3) +\n  \n  # Formatting\n  xlim(0, 4) +\n  ylim(0, 2) +\n  labs(title = paste(\"Convex Hull of\", n_points, \"Random Points\"),\n       subtitle = paste(\"Number of extreme points:\", n_extreme_points),\n       x = \"X coordinate\", \n       y = \"Y coordinate\") +\n  theme_minimal() +\n  theme(panel.grid.minor = element_blank(),\n        plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5)) +\n  coord_fixed()  # Maintain aspect ratio\n\n# Display the plot\nprint(p)\n\n\n\n\n\n\n\n\n\nCode\n# Print summary information\ncat(\"Summary:\\n\")\n\n\nSummary:\n\n\nCode\ncat(\"Total points generated:\", n_points, \"\\n\")\n\n\nTotal points generated: 100 \n\n\nCode\ncat(\"Number of extreme points (convex hull vertices):\", n_extreme_points, \"\\n\")\n\n\nNumber of extreme points (convex hull vertices): 15 \n\n\nCode\ncat(\"Proportion of extreme points:\", round(n_extreme_points/n_points, 3), \"\\n\")\n\n\nProportion of extreme points: 0.15 \n\n\nCode\n# Save the plot file (uncomment as necessary)\n# ggsave(\"convex_hull_plot.png\", plot = p, width = 8, height = 6, units = \"in\")"
  },
  {
    "objectID": "analysis/01-convex-hulls.html#code-for-figures-2-and-3-that-illustrate-the-theorem-for-different-distributions",
    "href": "analysis/01-convex-hulls.html#code-for-figures-2-and-3-that-illustrate-the-theorem-for-different-distributions",
    "title": "Convex Hull Theorem Illustration",
    "section": "Code for Figures 2 and 3 that illustrate the theorem for different distributions",
    "text": "Code for Figures 2 and 3 that illustrate the theorem for different distributions\nThis shows the generation of the figures for the convex hull verifications in section 2.2. Added on is the case of uniform points on a disk.\n\n\nCode\n# Load required libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(gridExtra)\nlibrary(geometry)\nlibrary(MASS)  # for mvrnorm\nlibrary(sp)    # for point.in.polygon\n\n\n\n\nCode\n# Generate points from uniform distribution on a disk\ngenerate_uniform_disk_points &lt;- function(n, d, radius = 1) {\n  if (d != 2) {\n    stop(\"Uniform disk distribution is only implemented for d = 2\")\n  }\n  \n  # Generate angles uniformly between 0 and 2π\n  theta &lt;- runif(n, 0, 2*pi)\n  \n  # Generate radii with r ~ sqrt(U) for uniformity\n  # (in 2D, need r ~ U^(1/2) for uniform distribution in disk)\n  r &lt;- radius * sqrt(runif(n))\n  \n  # Convert to Cartesian coordinates\n  x &lt;- r * cos(theta)\n  y &lt;- r * sin(theta)\n  \n  return(cbind(x, y))\n}"
  },
  {
    "objectID": "analysis/01-convex-hulls.html#generation-functions-for-different-distributions",
    "href": "analysis/01-convex-hulls.html#generation-functions-for-different-distributions",
    "title": "Convex Hull Theorem Illustration",
    "section": "Generation functions for different distributions",
    "text": "Generation functions for different distributions\n\n\nCode\n# Function to generate points from different distributions\ngenerate_distribution_points &lt;- function(n, d, distribution, correlation = 0.5) {\n  if(distribution == \"uniform_rectangle\") {\n    # Uniform in [0,4] x [0,2] (only for d=2) or [0,4] x [0,2] x [0,1] (for d=3)\n    if(d == 2) {\n      X &lt;- cbind(runif(n, 0, 4), runif(n, 0, 2))\n    } else if(d == 3) {\n      X &lt;- cbind(runif(n, 0, 4), runif(n, 0, 2), runif(n, 0, 1))\n    }\n  } else if (distribution == \"uniform_disk\") {\n   # Uniform distribution on disk\n  if(d == 2) {\n    # 2D disk implementation\n    theta &lt;- runif(n, 0, 2*pi)\n    r &lt;- sqrt(runif(n))\n    x &lt;- r * cos(theta)\n    y &lt;- r * sin(theta)\n    return(cbind(x, y))\n  } else if(d == 3) {\n    # 3D ball implementation\n    # Generate points from a 3D ball using spherical coordinates\n    theta &lt;- runif(n, 0, 2*pi)      # Azimuthal angle\n    phi &lt;- acos(2*runif(n) - 1)     # Polar angle\n    r &lt;- runif(n)^(1/3)             # Radius (cube root for uniform distribution)\n    \n    x &lt;- r * sin(phi) * cos(theta)\n    y &lt;- r * sin(phi) * sin(theta)\n    z &lt;- r * cos(phi)\n    \n    return(cbind(x, y, z))\n  }  }   else if(distribution == \"gaussian_independent\") {\n    # Independent standard Gaussians\n    X &lt;- matrix(rnorm(n * d, 0, 1), ncol = d)\n  } else if(distribution == \"gaussian_correlated\") {\n    # Correlated Gaussians with correlation r\n    Sigma &lt;- matrix(correlation, nrow = d, ncol = d)\n    diag(Sigma) &lt;- 1\n    X &lt;- mvrnorm(n, mu = rep(0, d), Sigma = Sigma)\n  }\n  return(X)\n}"
  },
  {
    "objectID": "analysis/01-convex-hulls.html#estimate-dn-by-monte-carlo",
    "href": "analysis/01-convex-hulls.html#estimate-dn-by-monte-carlo",
    "title": "Convex Hull Theorem Illustration",
    "section": "Estimate Dn by Monte Carlo",
    "text": "Estimate Dn by Monte Carlo\n\n\nCode\n# Function to estimate D_n using Monte Carlo\nestimate_D_n &lt;- function(hull_points, distribution, d, correlation = 0.5, n_test = 5000) {\n  # Generate test points from the same distribution\n  test_points &lt;- generate_distribution_points(n_test, d, distribution, correlation)\n  \n  if(d == 2) {\n    # Use point.in.polygon for 2D\n    if(nrow(hull_points) &lt; 3) return(1)  # If no proper hull, D_n = 1\n    \n    # Ensure hull is closed\n    if(!all(hull_points[1,] == hull_points[nrow(hull_points),])) {\n      hull_points &lt;- rbind(hull_points, hull_points[1,])\n    }\n    \n    inside_count &lt;- sum(point.in.polygon(test_points[,1], test_points[,2], \n                                        hull_points[,1], hull_points[,2]) &gt; 0)\n    D_n &lt;- 1 - inside_count / n_test\n    \n  } else if(d == 3) {\n    # Simple approximation for 3D using bounding box\n    if(nrow(hull_points) &lt; 4) return(1)\n    \n    bbox_min &lt;- apply(hull_points, 2, min)\n    bbox_max &lt;- apply(hull_points, 2, max)\n    \n    # Count points inside bounding box (conservative approximation)\n    inside_bbox &lt;- apply(test_points, 1, function(pt) {\n      all(pt &gt;= bbox_min) && all(pt &lt;= bbox_max)\n    })\n    \n    # For Gaussian distributions, estimate the total \"relevant\" measure\n    if(distribution == \"uniform_rectangle\") {\n      total_measure &lt;- 4 * 2 * 1  # [0,4] x [0,2] x [0,1]\n      inside_count &lt;- sum(inside_bbox)\n      D_n &lt;- 1 - inside_count / n_test\n    } else if(distribution == \"uniform_disk\") {\n      # For uniform disk, same approach as above\n      inside_count &lt;- sum(inside_bbox)\n      D_n &lt;- 1 - inside_count / n_test\n    }  else {\n      # For Gaussians, use a more conservative estimate\n      # Assume most probability mass is within some reasonable bounds\n      center &lt;- colMeans(hull_points)\n      inside_count &lt;- sum(apply(test_points, 1, function(pt) {\n        sqrt(sum((pt - center)^2)) &lt;= 0.6 * sqrt(sum((bbox_max - bbox_min)^2))\n      }))\n      D_n &lt;- 1 - inside_count / n_test\n    }\n  }\n  \n  return(max(0, min(1, D_n)))  # Clamp to [0,1]\n}"
  },
  {
    "objectID": "analysis/01-convex-hulls.html#illustration-of-the-main-theorem",
    "href": "analysis/01-convex-hulls.html#illustration-of-the-main-theorem",
    "title": "Convex Hull Theorem Illustration",
    "section": "Illustration of the main theorem",
    "text": "Illustration of the main theorem\n\n\nCode\n# Main simulation function for the theorem\nsimulate_main_theorem &lt;- function(n, d, distribution, correlation = 0.5, n_simulations = 300) {\n  results &lt;- data.frame(\n    simulation = integer(n_simulations),\n    V_n = integer(n_simulations),\n    D_n = numeric(n_simulations),\n    D_n_minus_1 = numeric(n_simulations),\n    V_n_over_n = numeric(n_simulations),\n    difference_squared = numeric(n_simulations),\n    D_diff = numeric(n_simulations),\n    n = integer(n_simulations),\n    d = integer(n_simulations),\n    distribution = character(n_simulations),\n    stringsAsFactors = FALSE\n  )\n  \n  for(i in 1:n_simulations) {\n    # Generate n points\n    X_n &lt;- generate_distribution_points(n, d, distribution, correlation)\n    \n    # Generate n-1 points for D_{n-1}\n    X_n_minus_1 &lt;- X_n[1:(n-1), ]\n    \n    # Compute convex hulls\n    if(d == 2) {\n      # For n points\n      hull_indices_n &lt;- chull(X_n[,1], X_n[,2])\n      V_n &lt;- length(hull_indices_n)\n      hull_points_n &lt;- X_n[hull_indices_n, ]\n      \n      # For n-1 points\n      hull_indices_n_minus_1 &lt;- chull(X_n_minus_1[,1], X_n_minus_1[,2])\n      hull_points_n_minus_1 &lt;- X_n_minus_1[hull_indices_n_minus_1, ]\n      \n    } else if(d == 3) {\n      # For n points\n      hull_faces_n &lt;- convhulln(X_n, options = \"Pp\")\n      V_n &lt;- length(unique(as.vector(hull_faces_n)))\n      hull_vertices_n &lt;- X_n[unique(as.vector(hull_faces_n)), ]\n      \n      # For n-1 points\n      hull_faces_n_minus_1 &lt;- convhulln(X_n_minus_1, options = \"Pp\")\n      hull_vertices_n_minus_1 &lt;- X_n_minus_1[unique(as.vector(hull_faces_n_minus_1)), ]\n    }\n    \n    # Estimate D_n and D_{n-1}\n    if(d == 2) {\n      D_n &lt;- estimate_D_n(hull_points_n, distribution, d, correlation)\n      D_n_minus_1 &lt;- estimate_D_n(hull_points_n_minus_1, distribution, d, correlation)\n    } else {\n      D_n &lt;- estimate_D_n(hull_vertices_n, distribution, d, correlation)\n      D_n_minus_1 &lt;- estimate_D_n(hull_vertices_n_minus_1, distribution, d, correlation)\n    }\n    \n    # Compute the quantities in the theorem\n    V_n_over_n &lt;- V_n / n\n    difference_squared &lt;- (V_n_over_n - D_n_minus_1)^2\n    D_diff &lt;- abs(D_n - D_n_minus_1)\n    \n    results[i, ] &lt;- list(i, V_n, D_n, D_n_minus_1, V_n_over_n, \n                        difference_squared, D_diff, n, d, distribution)\n    \n    #if(i %% 50 == 0) cat(\"Completed\", i, \"of\", n_simulations, \"simulations\\n\")\n  }\n  \n  return(results)\n}"
  },
  {
    "objectID": "analysis/01-convex-hulls.html#run-simulations-for-sample-sizes-from-50-to-600.",
    "href": "analysis/01-convex-hulls.html#run-simulations-for-sample-sizes-from-50-to-600.",
    "title": "Convex Hull Theorem Illustration",
    "section": "Run simulations for sample sizes from 50 to 600.",
    "text": "Run simulations for sample sizes from 50 to 600.\n\n\nCode\n# Set seed for reproducibility\nset.seed(20421)\n\n# Simulation parameters\nsample_sizes &lt;- c(50, 100, 200, 400, 600)\ndimensions &lt;- c(2, 3)\ndistributions &lt;- c(\"uniform_rectangle\", \"uniform_disk\", \"gaussian_independent\", \"gaussian_correlated\")\ncorrelation &lt;- 0.8\nn_sims &lt;- 250\n\n#cat(\"Running main theorem simulations...\\n\")\n\n# Run all simulations\nall_theorem_data &lt;- data.frame()\n\nfor(d in dimensions) {\n  for(dist in distributions) {\n    for(n in sample_sizes) {\n      #cat(\"\\n--- Running\", dist, \"d =\", d, \"n =\", n, \"---\\n\")\n      \n      sim_data &lt;- simulate_main_theorem(n, d, dist, correlation, n_sims)\n      all_theorem_data &lt;- rbind(all_theorem_data, sim_data)\n    }\n  }\n}\n\n# Clean up distribution labels\nall_theorem_data$distribution_clean &lt;- case_when(\n  all_theorem_data$distribution == \"uniform_rectangle\" ~ \"Uniform Rectangle\",\n  all_theorem_data$distribution == \"uniform_disk\" ~ \"Uniform Disk\",\n  all_theorem_data$distribution == \"gaussian_independent\" ~ \"Independent Gaussian\",\n  all_theorem_data$distribution == \"gaussian_correlated\" ~ paste0(\"Corr. Gaussian (r=\", correlation, \")\"),\n  TRUE ~ all_theorem_data$distribution\n)\n\ncat(\"\\nSimulations complete. Creating plots...\\n\")\n\n\n\nSimulations complete. Creating plots...\n\n\nCode\n# Compute summary statistics\ntheorem_summary &lt;- all_theorem_data %&gt;%\n  group_by(distribution_clean, d, n) %&gt;%\n  summarise(\n    mean_squared_diff = mean(difference_squared),\n    theoretical_bound = (8*d + 9) / n,\n    mean_D_diff = mean(D_diff),\n    theoretical_D_bound = (d + 1) / n,\n    mean_V_n_over_n = mean(V_n_over_n),\n    mean_D_n_minus_1 = mean(D_n_minus_1),\n    sd_squared_diff = sd(difference_squared),\n    .groups = 'drop'\n  )"
  },
  {
    "objectID": "analysis/01-convex-hulls.html#creation-of-figure-2",
    "href": "analysis/01-convex-hulls.html#creation-of-figure-2",
    "title": "Convex Hull Theorem Illustration",
    "section": "Creation of Figure 2",
    "text": "Creation of Figure 2\n\n\nCode\n# FIGURE 2: Main theorem bound - MSE\nfig2 &lt;- ggplot(theorem_summary, aes(x = n)) +\n  geom_line(aes(y = mean_squared_diff, color = distribution_clean), size = 1) +\n  geom_line(aes(y = theoretical_bound), linetype = \"dashed\", color = \"black\", size = 1) +\n  facet_wrap(~paste(\"d =\", d), scales = \"free_y\") +\n  labs(x = \"Sample size (n)\",\n       y = \"Mean squared error\",\n       color = \"Distribution\") +\n  theme_minimal() +\n  theme(legend.position = c(0.15, 0.35),\n        legend.justification = c(\"left\",\"bottom\"))+\n  coord_cartesian(xlim = c(100, max(theorem_summary$n)))+\n  scale_y_log10() \n  #+\n  #+annotate(\"text\", x = 400, y = 0.1, label = \"Theoretical bound\", \n  #         color = \"black\", size = 3)\n\nprint(fig2)\n\n\n\n\n\n\n\n\n\nCode\n# ggsave(\"main_theorem_mse_bound.png\", fig2, width = 7, height = 4,dpi = 300, bg = \"white\")"
  },
  {
    "objectID": "analysis/01-convex-hulls.html#second-part-of-theorem-figure-showing-d_n-difference",
    "href": "analysis/01-convex-hulls.html#second-part-of-theorem-figure-showing-d_n-difference",
    "title": "Convex Hull Theorem Illustration",
    "section": "Second part of theorem, Figure showing D_n difference",
    "text": "Second part of theorem, Figure showing D_n difference\n\n\nCode\n# FIGURE 3: Second part of theorem - D_n difference\nfig3 &lt;- ggplot(theorem_summary, aes(x = n)) +\n  geom_line(aes(y = mean_D_diff, color = distribution_clean), linewidth = 1) +\n  geom_line(aes(y = theoretical_D_bound), linetype = \"dashed\", color = \"black\", size = 1) +\n  facet_wrap(~paste(\"d =\", d), scales = \"free_y\") +\n  labs(x = \"Sample size (n)\",\n       y = \"Expected value of the differences in Ds\",\n       color = \"Distribution\") +\n  theme_minimal() +\n  theme(legend.position = c(0.17,0.45),\n        legend.justification = c(\"left\",\"bottom\"))+\n  coord_cartesian(xlim = c(100, max(theorem_summary$n)))+\n  scale_y_log10() #+\n # annotate(\"text\", x = 400, y = 0.01, label = \"Theoretical bound\", \n #           color = \"black\", size = 3)\n\nprint(fig3)\n\n\n\n\n\n\n\n\n\nCode\n# To save to a file\n# ggsave(\"main_theorem_D_diff_bound.png\", fig3, \n  #       width = 7, height = 4, dpi = 300, bg = \"white\")"
  },
  {
    "objectID": "analysis/01-convex-hulls.html#convergence-of-expectations",
    "href": "analysis/01-convex-hulls.html#convergence-of-expectations",
    "title": "Convex Hull Theorem Illustration",
    "section": "Convergence of expectations",
    "text": "Convergence of expectations\n\n\nCode\n# FIGURE 4: Convergence of expectations\nfig4 &lt;- ggplot(theorem_summary, aes(x = n)) +\n  geom_line(aes(y = mean_V_n_over_n, color = paste(distribution_clean, expression(-V[n]/n))), \n            size = 1.5, alpha = 0.7) +\n  geom_line(aes(y = mean_D_n_minus_1, color = paste(distribution_clean, expression(-D[n-1])\n)), \n            size = 1.5, alpha = 0.7) +\n  facet_wrap(~paste(\"d =\", d), scales = \"free_y\") +\n  labs(#title = expression(\"Convergence: \" * E[V[n]/n] * \" and \" * E[D[n-1]]),\n       #subtitle = \"The theorem states these should be equal in expectation\",\n       x = \"Sample size (n)\",\n       y = \"Expected value\",\n       color = \"Quantity\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  guides(color = guide_legend(ncol = 2))\n\nprint(fig4)\n\n\n\n\n\n\n\n\n\nCode\n#ggsave(\"main_theorem_convergence.png\", fig4, \n#        width = 6, height = 4, dpi = 300, bg = \"white\")\n\n\n# Print summary statistics\ncat(\"\\n=== MAIN THEOREM VERIFICATION ===\\n\")\n\n\n\n=== MAIN THEOREM VERIFICATION ===\n\n\nCode\nprint(theorem_summary)\n\n\n# A tibble: 10,000 × 10\n   distribution_clean         d     n mean_squared_diff theoretical_bound\n   &lt;chr&gt;                  &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n 1 Corr. Gaussian (r=0.8)     2    50           0.00255               0.5\n 2 Corr. Gaussian (r=0.8)     2    50           0.00255               0.5\n 3 Corr. Gaussian (r=0.8)     2    50           0.00255               0.5\n 4 Corr. Gaussian (r=0.8)     2    50           0.00255               0.5\n 5 Corr. Gaussian (r=0.8)     2    50           0.00255               0.5\n 6 Corr. Gaussian (r=0.8)     2    50           0.00255               0.5\n 7 Corr. Gaussian (r=0.8)     2    50           0.00255               0.5\n 8 Corr. Gaussian (r=0.8)     2    50           0.00255               0.5\n 9 Corr. Gaussian (r=0.8)     2    50           0.00255               0.5\n10 Corr. Gaussian (r=0.8)     2    50           0.00255               0.5\n# ℹ 9,990 more rows\n# ℹ 5 more variables: mean_D_diff &lt;dbl&gt;, theoretical_D_bound &lt;dbl&gt;,\n#   mean_V_n_over_n &lt;dbl&gt;, mean_D_n_minus_1 &lt;dbl&gt;, sd_squared_diff &lt;dbl&gt;\n\n\nCode\ncat(\"\\nBound violations for MSE:\\n\")\n\n\n\nBound violations for MSE:\n\n\nCode\nmse_violations &lt;- theorem_summary[theorem_summary$mean_squared_diff &gt; theorem_summary$theoretical_bound, ]\nif(nrow(mse_violations) &gt; 0) {\n  print(mse_violations[, c(\"distribution_clean\", \"d\", \"n\", \"mean_squared_diff\", \"theoretical_bound\")])\n} else {\n  cat(\"None! The MSE bound holds in all cases.\\n\")\n}\n\n\nNone! The MSE bound holds in all cases.\n\n\nCode\ncat(\"\\nBound violations for D difference:\\n\")\n\n\n\nBound violations for D difference:\n\n\nCode\nD_violations &lt;- theorem_summary[theorem_summary$mean_D_diff &gt; theorem_summary$theoretical_D_bound, ]\nif(nrow(D_violations) &gt; 0) {\n  print(D_violations[, c(\"distribution_clean\", \"d\", \"n\", \"mean_D_diff\", \"theoretical_D_bound\")])\n} else {\n  cat(\"None! The D difference bound holds in all cases.\\n\")\n}\n\n\nNone! The D difference bound holds in all cases.\n\n\nCode\ncat(\"\\nExpectation equality check (should be close to 0):\\n\")\n\n\n\nExpectation equality check (should be close to 0):\n\n\nCode\nexpectation_diff &lt;- theorem_summary %&gt;%\n  mutate(expectation_difference = abs(mean_V_n_over_n - mean_D_n_minus_1)) %&gt;%\n  arrange(desc(expectation_difference))\n\n# Print just the relevant columns\nprint(head(expectation_diff[, c(\"distribution_clean\", \"d\", \"n\", \"expectation_difference\")], 10))\n\n\n# A tibble: 10 × 4\n   distribution_clean     d     n expectation_difference\n   &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt;                  &lt;dbl&gt;\n 1 Uniform Disk           3    50                  0.397\n 2 Uniform Disk           3    50                  0.397\n 3 Uniform Disk           3    50                  0.397\n 4 Uniform Disk           3    50                  0.397\n 5 Uniform Disk           3    50                  0.397\n 6 Uniform Disk           3    50                  0.397\n 7 Uniform Disk           3    50                  0.397\n 8 Uniform Disk           3    50                  0.397\n 9 Uniform Disk           3    50                  0.397\n10 Uniform Disk           3    50                  0.397\n\n\nCode\ncat(\"\\n=== SUMMARY ===\\n\")\n\n\n\n=== SUMMARY ===\n\n\nCode\ncat(\"This code illustrates the main theorem with:\\n\")\n\n\nThis code illustrates the main theorem with:\n\n\nCode\ncat(\"1. MSE bound: E[(V_n/n - D_{n-1})^2] ≤ (8d+10)/n\\n\")\n\n\n1. MSE bound: E[(V_n/n - D_{n-1})^2] ≤ (8d+10)/n\n\n\nCode\ncat(\"2. D difference bound: E[|D_n - D_{n-1}|] ≤ (d+1)/n\\n\")\n\n\n2. D difference bound: E[|D_n - D_{n-1}|] ≤ (d+1)/n\n\n\nCode\ncat(\"3. Expectation equality: E[V_n/n] = E[D_{n-1}]\\n\")\n\n\n3. Expectation equality: E[V_n/n] = E[D_{n-1}]\n\n\nCode\ncat(\"4. Three distributions across 2D and 3D\\n\")\n\n\n4. Three distributions across 2D and 3D"
  },
  {
    "objectID": "analysis/01-convex-hulls.html#corollary-illustration-with-creation-of-fig-4",
    "href": "analysis/01-convex-hulls.html#corollary-illustration-with-creation-of-fig-4",
    "title": "Convex Hull Theorem Illustration",
    "section": "Corollary Illustration with creation of Fig 4",
    "text": "Corollary Illustration with creation of Fig 4\n\n\nCode\n# Load required libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(gridExtra)\nlibrary(geometry)\n\n# Function to simulate the corollary bound - with 4-simplex added\nsimulate_corollary_with_simplex &lt;- function(n, d, K_type = \"unit_cube\", n_simulations = 400) {\n  \n  # Define the convex set K and its true volume\n  if(K_type == \"unit_cube\") {\n    vol_K_true &lt;- 1\n    generate_points &lt;- function(n) matrix(runif(n * d, 0, 1), ncol = d)\n  } else if(K_type == \"unit_ball\") {\n    if(d == 2) {\n      vol_K_true &lt;- pi\n      generate_points &lt;- function(n) {\n        points &lt;- matrix(0, nrow = n, ncol = 2)\n        count &lt;- 0\n        while(count &lt; n) {\n          candidate &lt;- runif(2, -1, 1)\n          if(sum(candidate^2) &lt;= 1) {\n            count &lt;- count + 1\n            points[count, ] &lt;- candidate\n          }\n        }\n        return(points)\n      }\n    } else if(d == 3) {\n      vol_K_true &lt;- 4*pi/3\n      generate_points &lt;- function(n) {\n        points &lt;- matrix(0, nrow = n, ncol = 3)\n        count &lt;- 0\n        while(count &lt; n) {\n          candidate &lt;- runif(3, -1, 1)\n          if(sum(candidate^2) &lt;= 1) {\n            count &lt;- count + 1\n            points[count, ] &lt;- candidate\n          }\n        }\n        return(points)\n      }\n    }\n  } else if(K_type == \"simplex\") {\n    if(d == 2) {\n      # Triangle: vertices (0,0), (1,0), (0,1)\n      vol_K_true &lt;- 0.5\n      generate_points &lt;- function(n) {\n        u1 &lt;- runif(n)\n        u2 &lt;- runif(n)\n        x &lt;- ifelse(u1 + u2 &lt;= 1, u1, 1 - u1)\n        y &lt;- ifelse(u1 + u2 &lt;= 1, u2, 1 - u2)\n        return(cbind(x, y))\n      }\n    } else if(d == 3) {\n      # Regular tetrahedron (4-simplex in 3D)\n      # Vertices: (0,0,0), (1,0,0), (0,1,0), (0,0,1)\n      vol_K_true &lt;- 1/6  # Volume of tetrahedron with these vertices\n      generate_points &lt;- function(n) {\n        # Generate uniform points in 3-simplex using Dirichlet method\n        # Generate 4 exponential random variables, normalize to get Dirichlet\n        points &lt;- matrix(0, nrow = n, ncol = 3)\n        for(i in 1:n) {\n          # Generate 4 exponential(1) random variables\n          exp_vars &lt;- rexp(4, rate = 1)\n          # Normalize to get Dirichlet(1,1,1,1) - uniform on 3-simplex\n          dirichlet_vars &lt;- exp_vars / sum(exp_vars)\n          # The first 3 coordinates give us a point in the tetrahedron\n          # (the 4th coordinate is implicit: 1 - sum of first 3)\n          points[i, ] &lt;- dirichlet_vars[1:3]\n        }\n        return(points)\n      }\n    }\n  }\n  \n  # Storage for individual simulation results\n  lhs_values &lt;- numeric(n_simulations)\n  vol_ratios &lt;- numeric(n_simulations)\n  vertex_terms &lt;- numeric(n_simulations)\n  V_n_values &lt;- integer(n_simulations)\n  \n  for(i in 1:n_simulations) {\n    # Step 1: Generate n random points in K\n    X &lt;- generate_points(n)\n    \n    # Step 2: Compute convex hull and V_n\n    if(d == 2) {\n      hull_indices &lt;- chull(X[,1], X[,2])\n      V_n &lt;- length(hull_indices)\n      vol_convhull &lt;- convhulln(X, options = \"FA\")$vol\n    } else if(d == 3) {\n      hull_faces &lt;- convhulln(X, options = \"Pp\")\n      V_n &lt;- length(unique(as.vector(hull_faces)))\n      vol_convhull &lt;- convhulln(X, options = \"FA\")$vol\n    }\n    \n    # Step 3: Compute the volume estimator\n    if(V_n &lt; n) {  # Avoid division by zero\n      vol_K_hat &lt;- vol_convhull / (1 - V_n/n)\n    } else {\n      lhs_values[i] &lt;- NA\n      vol_ratios[i] &lt;- NA\n      vertex_terms[i] &lt;- NA\n      V_n_values[i] &lt;- V_n\n      next\n    }\n    \n    # Step 4: Compute the components of the LHS\n    vertex_component &lt;- (n - V_n)^2 / ((8*d + 10) * n)\n    volume_component &lt;- (vol_K_hat / vol_K_true - 1)^2\n    \n    # Step 5: Compute the full LHS for this simulation\n    lhs_values[i] &lt;- vertex_component * volume_component\n    vol_ratios[i] &lt;- vol_K_hat / vol_K_true\n    vertex_terms[i] &lt;- vertex_component\n    V_n_values[i] &lt;- V_n\n    \n    #if(i %% 100 == 0) cat(\"Simulation\", i, \"of\", n_simulations, \"completed\\n\")\n  }\n  \n  # Remove NA values\n  valid_indices &lt;- !is.na(lhs_values) & is.finite(lhs_values)\n  \n  # Step 6: Compute the empirical expectation\n  empirical_expectation &lt;- mean(lhs_values[valid_indices])\n  \n  return(list(\n    n = n,\n    d = d,\n    K_type = K_type,\n    empirical_expectation = empirical_expectation,\n    individual_lhs = lhs_values[valid_indices],\n    vol_ratios = vol_ratios[valid_indices],\n    vertex_terms = vertex_terms[valid_indices],\n    V_n_values = V_n_values[valid_indices],\n    n_valid_sims = sum(valid_indices),\n    theoretical_bound = 1\n  ))\n}\n\n# Set seed for reproducibility\nset.seed(42)\n\n# Simulation parameters - simplified to focus on the nice cases\nsample_sizes &lt;- c(50, 100, 200, 400, 600)\ndimensions &lt;- c(2, 3)\n# Now using simplex for both dimensions: triangle in 2D, tetrahedron in 3D\nK_types &lt;- c(\"unit_cube\", \"unit_ball\", \"simplex\")\nn_sims &lt;- 500\n\n# cat(\"Running corollary simulations with 4-simplex...\\n\")\n\n# Run all simulations and collect results\nresults_list &lt;- list()\nsummary_data &lt;- data.frame()\n\nfor(d in dimensions) {\n  for(K_type in K_types) {\n    for(n in sample_sizes) {\n      #cat(\"\\n--- Running for\", K_type, \"d =\", d, \"n =\", n, \"---\\n\")\n      \n      result &lt;- simulate_corollary_with_simplex(n, d, K_type, n_sims)\n      results_list[[length(results_list) + 1]] &lt;- result\n      \n      # Add to summary\n      summary_data &lt;- rbind(summary_data, data.frame(\n        K_type = K_type,\n        d = d,\n        n = n,\n        empirical_expectation = result$empirical_expectation,\n        median_lhs = median(result$individual_lhs),\n        q95_lhs = quantile(result$individual_lhs, 0.95),\n        max_lhs = max(result$individual_lhs),\n        prop_exceed_1 = mean(result$individual_lhs &gt; 1),\n        mean_vol_ratio = mean(result$vol_ratios),\n        n_valid_sims = result$n_valid_sims\n      ))\n    }\n  }\n}\n\n# Clean up labels\nsummary_data$K_type_clean &lt;- case_when(\n  summary_data$K_type == \"unit_cube\" ~ \"Unit Cube\",\n  summary_data$K_type == \"unit_ball\" ~ \"Unit Ball\",\n  summary_data$K_type == \"simplex\" & summary_data$d == 2 ~ \"Triangle\",\n  summary_data$K_type == \"simplex\" & summary_data$d == 3 ~ \"Tetrahedron\",\n  TRUE ~ summary_data$K_type\n)\n\ncat(\"\\nSimulations complete. Creating plots...\\n\")\n\n\n\nSimulations complete. Creating plots...\n\n\nCode\n# FIGURE 1: The main result - empirical expectation vs theoretical bound\nfig1 &lt;- ggplot(summary_data, aes(x = n, y = empirical_expectation, color = K_type_clean)) +\n  geom_line(size = 1.5) +\n  geom_point(size = 2.5) +\n  geom_hline(yintercept = 1, color = \"red\", linetype = \"dashed\", size = 1.5) +\n  facet_wrap(~paste(\"d =\", d), scales = \"free_y\") +\n  labs(title = \"Corollary Verification: Empirical Expectation vs Theoretical Bound\",\n       subtitle = expression(\"Bound: \" * E[frac((n-V[n])^2, (8*d+10)*n) * (frac(hat(vol)(K), vol(K)) - 1)^2] &lt;= 1),\n       x = \"Sample size (n)\",\n       y = \"Empirical expectation of LHS\",\n       color = \"Convex set K\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5),\n        legend.position = \"bottom\") +\n  scale_color_manual(values = c(\"Unit Cube\" = \"blue\", \"Unit Ball\" = \"red\", \n                               \"Triangle\" = \"green\", \"Tetrahedron\" = \"purple\")) +\n  annotate(\"text\", x = 500, y = 0.9, label = \"Theoretical bound = 1\", \n           color = \"red\", size = 4)\n\n# print(fig1)\n# ggsave(\"corollary_with_simplex_main.pdf\", fig1, width = 12, height = 6)\n\n# FIGURE 4: Volume estimator performance comparison\nvol_estimator_data &lt;- data.frame()\nfor(i in 1:length(results_list)) {\n  result &lt;- results_list[[i]]\n  vol_estimator_data &lt;- rbind(vol_estimator_data, data.frame(\n    K_type = result$K_type,\n    d = result$d,\n    n = result$n,\n    mean_vol_ratio = mean(result$vol_ratios),\n    sd_vol_ratio = sd(result$vol_ratios)\n  ))\n}\n\nvol_estimator_data$K_type_clean &lt;- case_when(\n  vol_estimator_data$K_type == \"unit_cube\" ~ \"Unit Cube\",\n  vol_estimator_data$K_type == \"unit_ball\" ~ \"Unit Ball\",\n  vol_estimator_data$K_type == \"simplex\" & vol_estimator_data$d == 2 ~ \"Triangle\",\n  vol_estimator_data$K_type == \"simplex\" & vol_estimator_data$d == 3 ~ \"Tetrahedron\",\n  TRUE ~ vol_estimator_data$K_type\n)\n\nfig4 &lt;- ggplot(vol_estimator_data, aes(x = n, y = mean_vol_ratio, color = K_type_clean)) +\n  geom_line(size = 1.5) +\n  geom_point(size = 2) +\n  geom_ribbon(aes(ymin = mean_vol_ratio - 1.96*sd_vol_ratio/sqrt(n_sims), \n                  ymax = mean_vol_ratio + 1.96*sd_vol_ratio/sqrt(n_sims),\n                  fill = K_type_clean), alpha = 0.3) +\n  geom_hline(yintercept = 1, linetype = \"dotted\", color = \"black\", size = 1) +\n  facet_wrap(~paste(\"d =\", d), scales = \"free_y\") +\n  labs(#title = \"Volume Estimator Performance\",\n       #subtitle = expression(\"Bias of \" * hat(vol)(K) * \" = \" * frac(vol(conv(X[1],...,X[n])), 1 - V[n]/n)),\n       x = \"Sample size (n)\",\n       y = \"Ratio expected value to true value of vol(K)\",\n       color = \"Convex set K\",\n       fill = \"Convex set K\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5),\n        legend.position = \"bottom\") +\n  scale_color_manual(values = c(\"Unit Cube\" = \"blue\", \"Unit Ball\" = \"red\", \n                               \"Triangle\" = \"green\", \"Tetrahedron\" = \"purple\")) +\n  scale_fill_manual(values = c(\"Unit Cube\" = \"blue\", \"Unit Ball\" = \"red\", \n                              \"Triangle\" = \"green\", \"Tetrahedron\" = \"purple\"))\n\nprint(fig4)\n\n\n\n\n\n\n\n\n\nCode\n# ggsave(\"volume_estimator_with_simplex.pdf\", fig4, width = 6, height = 4)\n\n# Detailed comparison - log scale to see the small values better\n\nfigsimplex &lt;- ggplot(summary_data, aes(x = n, y = empirical_expectation, color = K_type_clean)) +\n  geom_line(size = 1.5) +\n  geom_point(size = 2.5) +\n  geom_hline(yintercept = 1, color = \"red\", linetype = \"dashed\", size = 1.5) +\n  facet_wrap(~paste(\"d =\", d), scales = \"free_y\") +\n  labs(title = \"Corollary Bound: Log Scale View\",\n       subtitle = \"Detailed view of how far below 1 the empirical expectations are\",\n       x = \"Sample size (n)\",\n       y = \"Empirical expectation of LHS (log scale)\",\n       color = \"Convex set K\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5),\n        legend.position = \"bottom\") +\n  scale_color_manual(values = c(\"Unit Cube\" = \"blue\", \"Unit Ball\" = \"red\", \n                               \"Triangle\" = \"green\", \"Tetrahedron\" = \"purple\")) +\n  scale_y_log10() +\n  annotate(\"text\", x = 500, y = 1, label = \"Theoretical bound = 1\", \n           color = \"red\", size = 4)\n\nprint(figsimplex)\n\n\n\n\n\n\n\n\n\nCode\n#ggsave(\"corollary_log_scale_view.pdf\", figsimplex, width = 8, height = 4)"
  },
  {
    "objectID": "analysis/01-convex-hulls.html#supplementary-plot-of-theoretical-versus-estimates",
    "href": "analysis/01-convex-hulls.html#supplementary-plot-of-theoretical-versus-estimates",
    "title": "Convex Hull Theorem Illustration",
    "section": "Supplementary plot of theoretical versus estimates",
    "text": "Supplementary plot of theoretical versus estimates\n\n\nCode\n# Print summary results\ncat(\"\\n=== COROLLARY VERIFICATION WITH SIMPLEX ===\\n\")\ncat(\"Theoretical bound: E[LHS] ≤ 1\\n\\n\")\n\nsummary_table &lt;- summary_data %&gt;%\n  select(K_type_clean, d, n, empirical_expectation, prop_exceed_1) %&gt;%\n  arrange(d, K_type_clean, n)\n\nprint(summary_table)\n\ncat(\"\\nMaximum empirical expectation by case:\\n\")\nmax_by_case &lt;- summary_data %&gt;%\n  group_by(K_type_clean, d) %&gt;%\n  summarise(\n    max_empirical_exp = max(empirical_expectation),\n    min_n_for_max = n[which.max(empirical_expectation)],\n    avg_empirical_exp = mean(empirical_expectation),\n    .groups = 'drop'\n  )\nprint(max_by_case)\n\ncat(\"\\nVolume estimator performance (bias from 1):\\n\")\nvol_bias &lt;- vol_estimator_data %&gt;%\n  group_by(K_type_clean, d) %&gt;%\n  summarise(\n    avg_bias = mean(mean_vol_ratio - 1),\n    max_bias = max(abs(mean_vol_ratio - 1)),\n    .groups = 'drop'\n  )\nprint(vol_bias)\n\ncat(\"\\nCases where empirical expectation &gt; 1:\\n\")\nviolations &lt;- summary_data[summary_data$empirical_expectation &gt; 1, ]\nif(nrow(violations) &gt; 0) {\n  print(violations[, c(\"K_type_clean\", \"d\", \"n\", \"empirical_expectation\")])\n} else {\n  cat(\"None! The bound holds in all cases.\\n\")\n}\n\ncat(\"\\n=== SUMMARY ===\\n\")\ncat(\"All empirical expectations are smaller than 1, suggesting the bound is quite conservative.\\n\")\ncat(\"The tetrahedron (4-simplex in 3D) case has been added.\\n\")\ncat(\"Volume estimator shows good performance across all convex sets.\\n\")\n\n\nThe tetrahedron case uses the standard 3-simplex with vertices at (0,0,0), (1,0,0), (0,1,0), (0,0,1) and generates uniform points using the Dirichlet distribution method.\nThe LHS values being quite smaller than 1 suggests the theoretical bound is quite conservative."
  }
]